(upbeat music) - Hello, and welcome to DSC 257: Unsupervised Learning. I'm the instructor, Sanjoy Dasgupta, and what I wanna begin with today is a high-level overview of the field of unsupervised learning. So machine learning has clearly been a huge success story. It seems to be used everywhere, and it seems to constantly be enabling all sorts of new technologies. But it's a big field. And so one interesting question is, what part of machine learning is really responsible for this success? So I think that a large part of it comes from supervised learning. Specifically supervised learning for very specific tasks and trained on large labeled datasets. So here, for example, we have a smattering of images from the ImageNet dataset. This is a huge dataset; it has 15 million images, and it has 20,000 different categories. When a neural net is trained on this data, performance is actually pretty good, okay? So it's an example of the success of supervised learning. But at the same time, there are a lot of things with this setup that are really questionable. The big problems are two, okay? The first is that where is all of this labeled data coming from? So let's say, for instance, that I want to build a classifier that takes an image of a bird and says what bird it is, okay? So the first thing I need are lots of bird pictures. And I can perhaps get these by scraping the internet. Okay? So perhaps I can get tens of thousands, maybe even hundreds of thousands, of bird pictures in this way. But then I need these pictures to be labeled. And that's gonna require finding somebody who's a bird expert to do this, okay? And if the dataset is large, tens of thousands or hundreds of thousands, then the process of labeling the data is gonna be very time consuming and also expensive. In many applications, it's really not feasible to get a labeled dataset of the sheer size that supervised learning methods seem to demand. So that's the first problem. The second is that let's say we do manage to put together a large labeled dataset, and we are able to train a classifier, and it works well. But this classifier has been trained for a highly specific task. What about related tasks? Okay, so let's say, for example, that we train a speech recognizer on speakers from North America. Will it also work well on other English speakers, say from the UK or from Australia? Probably not that well. Let's say we train an image classifier that takes a picture of an animal and says what animal it is, and we train it on some collection of animals. Can we also use it for other animals? Probably not. In fact, it seems that for even small variations on the original task we might need a new large labeled dataset. So these are all problems with the supervised learning framework. And in part, because of these kinds of problems, there's a great interest in trying to understand what can be done without labels. In other words, what can be done using purely unsupervised methods? One way to think about it is that a lot of the data that we are dealing with has got rich structure. Images have a very rich structure. DNA sequences have structure. Speech has structure. And this structure exists independent of any classification task that we might choose to attach to the data. So in purely unsupervised mode, maybe it's possible to recover the structure, to extract the structure, and to somehow exploit it, okay? So this is, in a sense, the goal of unsupervised learning. Now, this sounds like a tall order, but one of the sources of inspiration and encouragement is that humans seem to learn quite a lot, primarily in unsupervised mode. When a child is born, for the first year or two, it does receive some supervision. Its parents stop it from doing certain things and force it to do other things. But by and large, the child is taking in its environment, all the smells, the sounds, the sight, in unsupervised mode, and is using all of this information to build a representation of the world. And this representation is so rich that by the end of, say, the second year of the child's life, it is able to learn new concepts with amazing efficiency. If you show a two-year-old a picture of a platypus, and you say, "This is a platypus," the child will immediately learn the concept, perfectly and robustly, from just this one example. This is a level of efficiency that current machine learning systems just aren't able to achieve at all. Okay, so what is the child doing? What kind of representation is it constructing? These are interesting and unanswered questions, but they're one of the sources of inspiration for unsupervised learning. So let's spend a little bit of time thinking about some of the things we can do without labels. Okay. So what can we do given data that is unlabeled? Well, one very simple thing is to simply remember the data. So for instance, let's say we encounter some collection of animals. If we simply remember them, then in the future, when another animal shows up, we will be able to figure out if this animal is something new, something we should potentially be worried about, or whether it's an animal that we've seen many times in the past, and we have nothing to fear, okay? So the simple act of remembering has a lot of applications. Another thing we can do with unlabeled data is to summarize it. Now, the summary could be something quite simple, like just the mean and variance of the data, or it could be something more sophisticated, like a clustering of the data. So for instance, let's say we have a collection of data points like this. We could say that, well, the data can be summarized as forming three clusters. We have a cluster over here with this canonical representative. We have a cluster over here centered at this canonical representative. And we have a cluster over here centered at this point. And so what we've done is to summarize the entire dataset by these three canonical representatives. This can be useful in settings like, for example, let's say that an online newspaper has a large client base, say millions of clients, and it's keeping track of what articles they read. Now, the newspaper would like some summary of their customers, but there's just so many of them. So in a situation like this, the clustering could be very useful. It might turn out, for example, that there is a large cluster containing, say, 20% of the customers who just do the crossword. Or another large cluster that just looks at the politics and business sections. Okay? So clustering is a very popular way of summarizing large datasets. Another thing we can do without labels is to compress data, to take the data representation and to make it smaller. Okay? So for example, in personality testing, people are asked lots and lots of questions, say 50 or 100 questions, and then the answers to those 50 or 100 questions are used to summarize the person are compressed into a very short vector. Maybe with 5 to 10 numbers. The idea is that this compressed form, this 5 or 10-dimensional vector, captures the essence of the person's personality and can be used to predict the answers to all the original questions, to all 50 or 100 questions. So from the compressed form, we can get an approximate version of the original data. Now, one very popular form of compression is principle component analysis, which takes data in some high-dimensional space and compresses it by projecting it onto a lower-dimensional subspace. We'll also look at many non-linear methods of compression, like autoencoders. A fourth thing that we can do with unlabeled data is to model the distribution of the data, to fit a distribution to the data. And once we have a probability distribution, we can generate new points from it. In unsupervised learning, there's a wide range of probability models that are used. These range from very simple models, like the Gaussian, to much more sophisticated models, like Bayesian networks or variational autoencoders. In fact, a lot of the current excitement in machine learning is around complex generative models. The fifth thing that we can do on unlabeled data is to embed it into a different space. So let me explain what I mean by this. So let's say that our data looks like this. Okay? So the data lies on the surface that looks like a Swiss roll. Now, this is fine. The problem with this particular representation is that our normal notions of geometry, like Euclidean distance or dot product, don't really work well in the data space. For instance, let's look at these two points over here. So a point right here and a point right here. The Euclidean distance between them is quite small. And yet when we look at the overall data surface, those two points should actually be quite far away from each other. So what we could do in this case is to embed the data into a two-dimensional space to sort of unfold the manifold. And then those two points, well, one of them would come here, and one of them would come somewhere over here, and it would be clear that, indeed, they're very far apart. So the purpose of embedding is to take the original data and change the representation. To map it into a different space that, in a sense, brings out the structure in the data. And what we want in particular is that, in the embedded space, in this new space are normal notions of geometry like Euclidean distance work well. Okay, so we've talked about five things that can be done without any labels at all. And in this course, we are gonna be looking at all sorts of different unsupervised learning methods, and it can be useful to keep these five high-level principles in the back of your mind. And as we're going over new methods, to try and think to yourself, which of these applies? Are we remembering summarizing, compressing, generating, or embedding? In some cases, several of them might apply. For example, we'll be covering a clustering method called mixture of Gaussians, which is a way to cluster data, and therefore a form of summarization, but is at the same time a probability model, and can therefore be used to generate new data. 