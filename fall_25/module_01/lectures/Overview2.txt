(bright music) - Now that we've done an overview of the field of unsupervised learning, let's talk a little bit about the specifics of this course. So the purpose of the course is to provide an overview, a very comprehensive introduction to the field of unsupervised learning. We are gonna be covering lots of unsupervised learning methods, and throughout, there are five questions that we're always gonna be trying to address. First of all what precisely are the tasks that we are trying to solve? This arises because in unsupervised learning there's often a little bit of vagueness about precise goals. For example, when we're clustering data, what exactly do we mean by a good clustering? Part of this vagueness arises because we don't have labels that serve as a definite target. And because of this, we have to be extra careful to precisely define what exactly we are trying to achieve. In many cases, this means identifying exactly the type of structure that we are trying to uncover in the data. The second thing that we'll always think about is what are general principles for coming up with solutions? Now, these are principles like local search which is heavily used in clustering algorithms, for example, or like sampling Which is used in generative models, or spectral decomposition Which is heavily used in principle component analysis, singular value decomposition, and some of the dictionary learning and embedding methods that we'll be considering. So those are high level design principles. We'll also talk about the most commonly used algorithms for solving each individual problem. What are the pros and cons of each? We'll be interested also in the statistical properties of these unsupervised learning procedures. Now, unfortunately, unsupervised learning is an area where for the most part, there aren't individual algorithms that work well across the board. So what are the kinds of data on which they work well? What are the kinds of data on which they might fail? How many data points do we need in order to get a good solution? These are the kinds of things that we'll be thinking about. And finally, we'll look at typical use cases for each of the methods that we consider. Because we are talking about so many different unsupervised learning methods, we are gonna divide them into three categories. What we'll start with in part one of the course are basic primitives. These are methods that are quick and dirty. They're fast and very, very widely used. They include nearest neighbor simple statistics like mean variance, median, histograms and data sketches, clustering, simple compression methods like principle component analysis, and random projection. These are the methods that are used nonstop. They're very efficient, they're very convenient. The downside is that they typically don't allow a lot of flexibility in specifying the kind of structure that we are looking for. For example, they usually don't allow us to specify prior knowledge about the data. So in this sense, the methods, although fast and convenient are somewhat rigid in what they deliver. So this will be the first part of our course. In the second part, we'll move on to probabilistic models. These are much more powerful and much more expressive. And for the most part there's a nice sort of mathematical basis of this area. So we'll start by talking about basic distributions, exponential families like Gaussians and Prossaint and we'll talk a little bit about Bayesian methods of estimation. And then we'll move on to higher dimensional models like the multivariate Gaussian and undirected and directed graphical models. We'll also talk a lot about latent variable models and about methods for doing inference using probabilistic models. And these include variational methods and methods like sampling. In the final part of the course we'll move on to some methods that are able to capture more advanced types of structure in the data. And in fact, we'll end with two methods, auto encoders and self supervised learning, where the structure in the data is something that we can't even define. We don't have the words for it, but we'd like to capture it nonetheless, it sounds mysterious and it is mysterious and somehow these methods actually seem to work quite well. Now in terms of background for this course we'll be making heavy use of linear algebra, probability and statistics, and basic algorithmic analysis. And as always, in machine learning it will be important to have a good grasp of Python and of NuMPI and PsycLearn. So what are the concrete logistics for this class? What are the things to do? Well, before watching lectures, you should download slides. The slides have quite a few gaps in them and part of the learning process is filling in these gaps, filling in this information while watching the lecture. There'll be weekly assignments to be submitted by a grade scope, and if you have questions about any of this, you should come to office hours. So that's it. That's an overview of the class. I hope you enjoy it. 