(upbeat music) (air whooshing) - When you're dealing with high-dimensional data, it's almost inevitable that there will be dependencies between the dimensions. That is to say between the features. In a lot of unsupervised learning, it's crucial to be able to quantify these dependencies. And so what we look at now is some common measures of dependence. Now, the simplest case of all is when random variables are independent. What does that mean mathematically? Well, we say that X and Y are independent if this holds: if the probability that X takes on a particular value, little x, and simultaneously Y takes on a value, little y, is equal to the probability that X takes on that value times the probability that Y takes on the value little y. Another way to understand this is to divide both sides of the equation by the probability that Y equals little y. So let's divide by the probability that Y equals little y. On the left, we then get the probability that X takes on the value little x given that Y takes on the value little y. And on the right, we get the probability that X takes on the value little X. And so what independence means is that the probability that X takes on any given value does not depend on whatever value Y happens to have. Okay, so this is another way of writing down the condition that X and Y are independent. Now, let's do a little example. Over here, we have the joint distribution of two random variables, X and Y, that take values in the range minus one, zero and one. Okay? So what we have here is the joint distribution. That is the distribution of all pairs of values. So for instance, the probability that X is minus one and Y is minus one, in other words, the probability of the pair, minus one minus one is 0.4. So seeing that way, there are nine possible outcomes. There are nine possible pairs of values and their probabilities have to add up to one. Okay? So now the question is that we have this table that fully specifies the joint distribution of X and Y, are X and Y independent? Okay, so let's go back to our formula. So looking at the two sides of the equation, on the left, we need the joint distribution of X and Y, which we have. On the right, we need the distribution of X just by itself. That's called the marginal distribution of X. And we need the distribution of Y just by itself, the marginal distribution of Y. So let's go ahead and compute those two things. Let's start with the marginal distribution of X. Okay, so X can take on three possible values: minus one, zero, and plus one. What are the probabilities of them? Well, let's start with minus one. There are three ways in which X can be minus one, this, this, and this. Okay, because there are three possible values that Y can take on. And adding up those probabilities, we get a total of 0.8. So the probability that X equals minus one is 0.8. Likewise, the probability that X is zero, well, that can happen in three ways and those add up to 0.1. And the probability that X is one is also 0.1. So this is the marginal distribution of X. Let's do the same for Y. Let's find the marginal distribution of Y. So Y can be minus one, zero, or one, and the probability that it's minus one, well, that's any of these three cases in that column. And those add up to 0.5. The probability that Y is zero, that's the numbers in that column, and those add up to 0.2. And Y equals one in these three ways, that final column, and those probabilities add up to 0.3. Okay, so we have the marginal distributions of X and of Y, and now we can go back to this definition over here. Okay, so let's start with this entry. The probability that X is minus one and Y is simultaneously minus one. If X and Y are independent, that would be equal to this probability times that. And indeed it is. Okay, 0.8 times 0.5 equals 0.4. Now let's move on to the next item in the table, 0.16. If X and Y were independent, that would be equal to the probability that X is minus one times the probability that Y is zero, 0.8 times 0.2. And indeed that's equal to 0.16. Okay, so we are off to a good start, but we are not done. We have to check every single entry in this table in the same way. And when we do, we find that X and Y are indeed independent. Okay, so suppose we have samples from some distribution, some unknown distribution, and we are curious about whether X and Y are independent or not. So we get these pairs, these samples, X1 Y1, X2 Y2, X3 Y3 and so on. And we wanna know, are these two random variables, X and Y independent or not? How would we do that? Well, looking back at the definition over here, it look like we really need the joint distribution of X and Y. So what we would need to do is to estimate the probabilities of all possible pairs XY. So need to estimate the probability of all pairs X and Y. That seems like a tall order. It seems like it would require a lot of data even to get a very approximate estimate. And indeed, measuring independence, testing independence, is not easy at all. Okay, so now let's talk about measures of dependence. The more common case when we have two random variables is that there will be some dependence between them. So for example, let's say we choose a person at random from the population of San Diego, let's say, and we let H be the person's height and W be the person's weight, will they be independent? Well, if they were independent, it would mean that the probability of a particular height and simultaneously having a particular weight is equal to the probability of that height times the probability of that weight. But that's not true at all. Larger heights will tend to go with larger weights and so there's clearly some dependence between these two variables. So let's see what this looks like pictorially. So what we have over here in the scatterplot is data from 507 people, ages 20 to 30, who live in San Francisco, it happens. And each of these points is the data for one person, okay? So this person over here, for example, has got a height of a little under 200 centimeters and a weight of about 85 kilograms. Okay, so we have 507 points over here, each of which is the height and weight of a particular person. And you can see that there's a general upward trend. Higher heights tend to go with higher weights. The two variables are positively correlated. Now, there are three qualitative types of correlation. There's positive correlation, which is what we saw for heights and weights, where larger values of one variable tend to go with larger values of the other. And so the data seems to slope upwards. Mathematically, what this means is that the average value of height times weight is greater than the average height times the average weight. Okay? So pictorially, it looks like the points are sloping upwards and mathematically or more formally, this is what we mean by a positive correlation. Now, on the other hand, data could be negatively correlated. So when we have two random variables, X and Y, we say they're negatively correlated if this condition holds: if the average value of X times Y is less than the average X times the average Y, and pictorially it looks like the data is sloping downwards, that is to say, larger values of one feature tend to go with smaller values of the other feature. Or it could be the case that neither of these two happens and the expected value of XY is exactly equal to the expected value of X times the expected value of Y. If that's the case, we say that the two variables are uncorrelated, and pictorially, it looks like the data is neither sloping up nor down. So these are three qualitative types of correlation. But in order to really use these notions, we need to go beyond qualitative correlation and we need to make this quantitative. A common way of doing it is by the correlation coefficient. This is a single number in the range minus one to plus one that measures dependence. So we'll get to the formula for it in a second. But before we do that, what I'd like to do is to just show you some pictures of data sets that have different correlation coefficients. So over here we have a dataset that has a correlation coefficient of one, and that's something we think of as a perfect correlation. What you can see over here is that the data is on a line. There is a perfect linear relationship between the two variables. Next, we have a data set that has a smaller correlation coefficient of 0.75. So you can see that, again, it's sloping upwards but it's a little bit more of a cloud. It isn't quite aligned. And as the correlation drops down to zero, we get closer and closer to having data that just looks like a blob. And as we get to the negative correlation values, it's really just like the positive values but sloping downwards instead of upwards. Okay, so this is pictorially what different correlation coefficients look like. Let's see now how we define this mathematically. Okay, so let's say we have two random variables, X and Y, and we wanna capture the correlation or the dependence between them. So if you recall, we said that they are positively correlated. So we said positive correlation means that the expected value, the average value of X times Y is greater than the average X times the average Y. Okay? We said they're negatively correlated if the average value of X times Y is less than the average X times the average Y. And we said that they're uncorrelated if the average value of X times Y is equal to the average X times the average Y. So given these three qualitative notions, perhaps the most intuitive way to make this quantitative is to simply look at the difference between the two sides of this equation. To look at the expected value of XY minus the expected X times the expected Y, because we know that if that's a positive number, then there's positive correlation. If that's a negative number, there's negative correlation. And if that's zero, the two variables are uncorrelated, okay? And indeed, that's what we call the covariance between X and Y. So this is a very nice notion of dependence and it's one that we are gonna be using very heavily throughout the rest of the class. Now, this is a nice and intuitive notion, but one downside of it is that it's not in the range minus one to plus one. This covariance between X and Y could be as large as the standard deviation of X times the standard deviation of Y. And so if we wanna normalize it to get a number in the range minus one to plus one, what we should do is to take the covariance and divide by the standard deviations of the two variables. And when we do that, we get the correlation coefficient that we talked about earlier. So the correlation is just the covariance normalized by dividing by the standard deviations of the two variables. And this is something that's in the range minus one to plus one. Okay, so let's look at a little example. Here, we have the joint distribution of two variables, X and Y, and what we wanna figure out is the covariance and the correlation between them. Okay, so let's just write down the formulas so that we know what we need. So the covariance between X and Y is the average value of XY minus the average value of X times the average value of Y. And the correlation is the covariance normalized by dividing by the standard deviations of the two variables. Okay? So how can we compute these things? Well, let's start with X. In order to compute the average value of X, the expectation of X, and the standard deviation of X, we need the distribution of X just by itself. In other words, we need the marginal distribution of X. Okay, so let's do the marginal for X. So here we have X. It can take on only two values, plus one or minus one. The probability that it's plus one, well, that happens in these two rows, and that's a probability of 1/2. And so the probability that it's minus one must also be 1/2, okay, since the probabilities add up to one. And what that tells us is that the expected value of X is zero, and the variance of X is just the expected value of X squared, which is one. And so the standard deviation of X is also one. Good. So we have all the information for X. Now, let's go on and do Y. Okay, so let's look at the marginal for Y. And Y takes on two values, four or minus four. The probability that Y is four, well, that happens in this line and this line. So those probabilities add up to 3/8, and therefore the probability that Y is minus four must be 5/8. Okay? And that tells us that the expected value of Y is four times 3/8, minus four times 5/8. So that's minus one. And the variance of Y is the expected value of Y squared minus the expected value of Y, the whole thing squared, okay? Now, Y squared is always equal to 16. So the expected value of Y squared is 16 and we've already computed the mean of Y, that's one. And so the variance is 15 and therefore the standard deviation of Y is the square root of 15. Okay. So we have X and Y taken care of. Now we need the expected value of X times Y. So let's put that over here. X times Y can be either four or minus four. When is it four? Well, it's four in this line and in this line. So the probability that it's four is 5/8 and the probability that it's minus four must therefore be 3/8, and therefore the expected value of XY is four times 5/8 minus four times 3/8, which is one. Great. So now we can plug into our equations over here. We get that the covariance of XY is the expected value of XY, which is one, minus the expectation of X, which is zero, times the expectation of Y. So minus zero. So the covariance is one. And the correlation is just the covariance divided by the standard deviation of X and the standard deviation of Y. So the correlation is one over the square root of 15. Okay? So the two variables have a positive correlation but it's a relatively small correlation. The correlation coefficient is just one over the square root of 15, so roughly a quarter, that would count as a rather small correlation. So let's talk a little bit about independence versus uncorrelatedness. Are these the same things? Okay. So when we say that variables are independent, one doesn't depend on the other, it seems that the notion of uncorrelatedness is very similar. Okay? So what is the exact relationship between them? Well, let's say that X and Y are independent. It certainly means that they are uncorrelated. But the reverse need not be true. It could be, it's possible, to have XY be uncorrelated, but dependent. So the relationship only holds in one direction. Let's see an example of this, okay? So let's say we have X that takes on two possible values, minus one and plus one. And let's say Y takes on values minus one, zero, or one, okay? And let's draw the joint distribution over here. So let's say we have 1/4, 0, 1/4, and 1/2 over here. Okay. So that's the joint distribution, the probabilities add up to one as they should. So let's compute the correlation. Well, the expected value of X in this case, let's see, X is minus one with probability 1/2 and plus one with probability 1/2 so the expected value of X is zero. What about the expected value of Y? Well, Y is minus one with probability 1/4, zero with probability 1/2, and one with probability 1/4. It's symmetric around zero. So the expected value of Y is zero as well. Okay? And what about the expected value of XY? Well, what values does X times Y take on? In the first row, X times Y is zero. In the second row, X times Y is zero so it seems that X times Y is always zero. Okay? And so the expected value of XY is a zero. And so we get the covariance between XY is zero. So the two variables are uncorrelated but there's a dependence between them. When X is equal to plus one, we see that Y has to be equal to zero, okay? So when X is plus one, the only possibility for Y is zero and so we get a lot of information about Y. Similarly, when X is minus one, we know that Y cannot possibly be zero. So knowing the value of X tells us about Y and so these two are clearly not independent. It makes sense that independence and uncorrelatedness are not the same things. After all, we saw that measuring the testing for independence is really very difficult. On the other hand, testing for correlation is very easy. We just need to compute a few expected values. Okay, so that's it for measures of dependence. We'll using these pretty heavily throughout the remainder of the class. 