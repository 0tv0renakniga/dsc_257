(bright cheery music) - In unsupervised learning, there are no labels, and as a result, the geometry of the data space takes on central significance. One way to specify this geometry is to associate a distance function or similarity function with the data space. And so we're gonna spend a little bit of time talking about distances and similarities. So let's start with a family of distance functions called the LP norms. Now perhaps the most familiar distance function of all is Euclidean distance, simply because this is actual physical distance. So suppose we have two vectors, X and Z, and let's say that these are M dimensional vectors. So X has M coordinates, X one, X two, all the way to XM, and Z also has M coordinates. Then the Euclidean distance between them is obtained by looking at the difference along each coordinate, squaring this difference, adding it up over all M coordinates, and then taking the square root of the whole thing. This is Euclidean distance. Now it turns out that there's a whole family of somewhat similar looking distance functions and these are the LP distances or the LP norms. So here's the formula for LP distance, and there's an LP distance for every value of P greater than or equal to one. So for example, we could take P equal to one, or we could take P equal to 1.1, or even P equal to infinity. So in order to denote LP distance, we use the same double bars, but we use this subscript of little P to indicate which P we're referring to. Now in defining LP distance, once again, we look at the difference along each coordinate. We take the absolute value of that difference and then we raise it to the Pth power and we add this up over all the features, and finally, we raise the whole thing to the one over P power, which is the same as taking the Pth root of the whole thing. Now, when P equals two, this is exactly Euclidean distance. This is what we had before. And two other interesting cases, are when P equals one and P equals infinity. Okay, so for P equals one, we get L one distance, which is very natural. It's simply the absolute difference along each coordinate added up across all coordinates. An L infinity distance simply looks for the coordinate along which the two vectors differ the most and returns the difference along that coordinate. Okay, so let's look at a little example. Let's say that X is the 0.21, and let's say we have Z over here, which is the 0.55. Okay, so what are the L two, L one and L infinity distances? Well, in order to compute this, we have to look at the difference along each coordinate. So along the first coordinate, the difference is five minus two, which is three, and along the second coordinate, the difference is five minus one, which is four. Okay, so the L two distance is physical distance. So it's literally this length over here, and that's five. Okay, so in this case, the L two distance is five. The L one distance is the difference along one coordinate plus the difference along the other coordinate. So the L one distance is seven, and for the L infinity distance, we look at the coordinate along which the difference is the greatest, and in this case, it's the second coordinate. It's this one over here. So the L infinity distance is four, okay? So out of all the LP distances, these are the three we use the most, L one, L two, and L infinity. Let's look at another example over here. So let's say we have the all ones vector in RD. So that means we have some vector X and it's all ones, and there are D entries. And now the question is, what is its L two length and L one length and L infinity length? Now when we talk about the length of a vector, what we really mean is how far is it from the origin, from the all zeros vector? Okay, so let's compute that. So how far is X from the origin, the all zeros vector, in let's say L two distance? So it's the L two norm of X. And to compute this, we square all the coordinates. So it's one squared plus one squared, and we add them up, one squared. And so inside the square root we get a D. And so the L two norm is the square root of D. What about the L one norm? In this case, we just look at the absolute value of each coordinate, which is one, and we add them up. So we get one plus one and we get D of these. So the L one norm of X is D. And the L infinity norm? Well, we simply look at the coordinate along which the number is the greatest, and in this case, all coordinates are equal. Okay, so we could pick any of them. And the L infinity norm of X is one. Let's look at another example. So here we are as defined all points in R two, which is the plane, all points whose L two length is one. Okay, so let's see what that is. So we want the set of all points in the plane. So these points have two coordinates, X one and X two. Okay, so we want the set of all points, X one and X two, whose L two length is one. So that means the L two length is X one squared plus X two squared, the square root of that, we want that to be equal to one, okay. That actually is the unit circle. Okay, so let's draw the plane over here, we have X one, we have X two, and this set of points is just the unit circle over here and it goes through one, one, minus one, minus one. So that's the unit ball for the L two norm, okay? So this is the first one. In the second case we're looking for points whose L one length is one, okay? So again, points in the plane, so X one and X two, and the L one length is the sum of the absolute values of the coordinates. So it's the absolute value of X one plus the absolute value of X two, and we want that to equal one, okay? So let's draw the plane again. So this is X one and X two. And let's look at this point over here, one, zero. The absolute value of the two coordinates is equal to one. Good, so that point is gonna be on there. Let's look at zero, one. Again, it's on there. Let's look at negative one, zero. The absolute values of the coordinates add up to one, okay? Let's look at zero, negative one. Again, that's on there, and in fact, this one, this unit ball looks like this, it looks like a diamond. Okay. So that's the L one unit ball, okay? Let's check one more point. So for example, if we look at this point, one half, one half, that lies on that line, okay? Because it's the midpoint between one zero and zero one. And sure enough, the absolute values of the two coordinates, a half and a half, add up to one. Okay, now the last one is the L infinity norm. So we want all points X one, X two, such that the larger of the two absolute values is equal to one, and in this case, it again goes through those same four points, but this time it looks like a box. So that's the unit ball for the L infinity norm. Now in some cases, it's useful to add weights to these norms. So let's return briefly to the ChemCam data, which we talked about earlier. Now here, each data point refers to an entire wavelength spectrum, and what we saw before was that each data point is a vector in 6,144 dimensions where each coordinate, each dimension is the intensity value at a particular wavelength. So in order to measure the distance between two such factors, we could certainly use the L one norm, but let's say that, as the scientist looking at this data, there are certain elements that we are not interested in. So for example, let's say we are not interested in sodium. What we would want to do then is to suppress or eliminate the wavelengths at which sodium characteristically appears. So in this case, it would be this wavelength over here, and this one over here. How can we do that, how can we suppress certain coordinates or certain wavelengths, in this case? One way to do this is to use a weighted L one norm, okay? So this is the same formula as the L one norm, but now we've thrown in these weights, and these are arbitrary non-negative numbers, okay? So if there's a particular feature that we want to disregard, we could just set the weight for that feature, W sub I, to zero, or we could do something more sophisticated where instead of entirely eliminating the feature, we just down weight it, we reduce its weight to maybe a half or something less, or we up weight it, okay? We can do something similar for any LP norm. How would we do this? Okay, so let's recall the formula for an LP norm. So the LP norm, between two vectors X and X prime, the LP distance between them, what was the formula again? Well, we sum overall coordinates, we look at the absolute difference along that coordinate, we raise it to the Pth power, we add those up, and then we take the Pth root of the whole thing. How would we do a weighted version of this? If we wanted to attach a weight to each feature, to each coordinate, where would the weight go? Well, we could just stick it in here, okay? So we can have weights, WI, that we put in there, and these weights could be specified by the person studying the data because they want to up weight certain features or down weight other features. There are also situations in which we feel we should be using weights, but we don't know exactly what the weights should be. And what we want to do is to actually learn the weights from data. This is called metric learning and it's an important part of unsupervised learning. 