(calm music) - Okay, so we have been talking a lot about distance functions in data space, but it seems like we could equally talk about similarity functions. For example when we are doing proximity search, we want to find the data point that has the smallest distance to a query point, but we could equivalently ask for the data point that has the highest similarity to the query. In this way, it seems like there's a dual relationship between similarities and distances. So what are some commonly used similarity functions? Let's take a look. One similarity function that's widely used in information retrieval is the Jaccard Similarity. And this is a notion of similarity between sets. Let's say for example that you ask Google a question like, "When was Napoleon born?" (pen scratching) This can be converted into a set of words, a bag of words representation containing the words in the sentence. So, when, was, Napoleon, born, the set of four words and now let's say that somewhere on the internet there is the sentence. "Napoleon was born in 1769." (pen scratching) The set corresponding to this is again the bag of words representation. Napoleon, was, born, in, 1769. Now these two sets the set corresponding to the query and the set corresponding to this sentence have a high overlap. And as a result, Google would flag this sentence as potentially containing the answer to the query. The way we measure the overlap between these two sets is using Jaccard Similarity. So the Jaccard Similarity between two sets A and B is the size of their intersection divided by the size of their union. In this case, the Jaccard Similarity is the number of words they have in common, which was, Napoleon, was, born. So they have three words in common, that's the size of their intersection. And the union is just the total number of words across these two sets, which is four and then two additional words. So six. So the Jaccard Similarity between these two sentences or more specifically the sets corresponding to these two sentences is a half. And that is a fairly high level of Jaccard Similarity. So a notion of similarity between sets. Now, in what range does this slide? Well, the largest it can be is if the two sets are identical in which case the Jaccard Similarity is one. And the smallest it can be is if the two sets are disjoint, in which case it's zero. So the Jaccard Similarity lies in the range from zero to one. Now, suppose I give you a set A, okay so A is some given set, for what set B is the similarity maximized? Well, it's maximized when B equals A. So it's maximized when B equals A. And in fact, for any other set B the similarity will be smaller. Okay, so when B equals A, the similarity equals one and for any other set B, the similarity will be smaller. Now, a second notion of similarity that's widely used is the Cosine Similarity. And this is a notion of similarity between vectors. So let's say we have two vectors, X and Z. The Cosine Similarity between them is the dot product between the two vectors, X dot Z divided by the length of X in L two norm, and the length of Z also in L two norm. And it turns out that this is intimately related to the angle between the two vectors. In fact, the Cosine Similarity is the cosine of the angle. So S of XZ is exactly the cosine of theta. So in what range does this lie? Well, when the two vectors are pointing in the same direction, the angle between them is zero and the cosine is zero is one. So the largest it could be is one and the smallest it can be is when the vectors are in opposite directions so that the angle between them is 180 degrees. In that case, the cosine is negative one. So the Cosine Similarity lies in the range from minus one to plus one. Now, suppose I give you a vector X and I ask for what Z Is the Cosine Similarity maximized? Well, it's gonna be maximized when the angle between the two vectors is zero. So it's gonna be maximized when Z equals X but actually Z doesn't have to equal X it could also be twice X. It just has to point in the same direction as X so that the angle between them is zero. So when Z equals X, or in fact any positive multiple of X. (pen scratching) And in all such cases, the Cosine Similarity will attain its maximum value of plus one. So this is a very useful notion of similarity between vectors. Now another notion of similarity between vectors is just the dot product of the two vectors. So in what range does this lie? Well, the dot product could actually be arbitrarily large or arbitrarily small. So it lies in the range minus infinity to plus infinity. Can the dot product between X and Z ever be larger than the dot product between X in itself? Okay, so let's say we have a vector X. Can we have a vector Z that's pointing in some other direction such that the dot product between X and Z is actually greater than the dot product between X in itself? We can certainly have that. For example, if we have Z looking like this the dot product between X and Z in this case is greater than the dot product between X and itself. And this is just by virtue of the fact that Z is so long even though it's pointing in another direction. So this is a little bit of a warning sign if we want to use dot product as a notion of similarity. This is something that should give us pause. 'cause we'd ideally hope that the similarity would be maximized when you have vectors that are identical to X or at least in the same direction as X. And we see that that's actually not the case. So one way to get around this is to restrict attention to vectors that all have the same length, for example, unit vectors. So the answer here is yes, and this is a problem (pen scratching) But we can alleviate this problem by restricting attention to vectors that all have the same length, (pen scratching) to vectors of the same length. For example, unit vectors, vectors of length one. (pen scratching) Okay, so now we've talked about three specific similarity functions, the Jaccard Similarity, the Cosine Similarity and the dot product. What about families of similarity functions? Just like we had families of distance functions they're similarly large families containing lots of similarity functions. Well, perhaps the most prominent such family is the family of Kernel Functions. And these really are a generalization of dot products. So let's say there are data line some space X and once again, this is an arbitrary space. It could be a space of strings or graphs or images and we have some similarity function on this space. What is it? It's a function that takes two objects and returns a similarity value. So we have some similarity function K. We'll say it's a Kernel Function if the following condition holds, if there is some mapping phi, if there's some embedding that maps objects, that maps data objects into a vector space, okay? There's some way to embed these objects script X into a vector space such that the similarity function between two objects X and Z is simply the dot product between the embeddings of those objects. So it's a generalized notion of dot product. And later in the course when we discuss this typically this embedding will be realized by a neural net. We'll have a neural net that takes some data object X and spits out an embedding of the object in a vector space. So below are some examples of Kernel Functions, okay? That can be realized as dot products in some embedded space. So that's it for now. We have talked a lot about the geometry of data space, in particular attaching distance and similarity functions to the data. 