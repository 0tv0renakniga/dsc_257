(upbeat music) (air whooshing) - Welcome back, everybody. So, so far we've talked a little bit about what you can achieve by merely remembering the data that you've seen. Our next topic is going to be summaries of data, and we are gonna begin with the very simplest statistical summaries: the mean, the median, the variance. So, these are familiar, they're widely used, they can be used on their own. And what we'll find is throughout this class, they will also be components of more sophisticated models that we build. So, let's take a little bit of time to review these basic definitions. So, let's start with the mean. Let's say we have a random variable, x. And let's say that x has got distribution P. So, the mean of x, the expected value of x, or equivalently the mean of the distribution, is intuitively the average value that x takes on. Now, if x is a discreet random variable, the mean is given by this formula over here, so it's a sum over all the possible values that x can take on. And what we sum up is that particular value weighted by the probability that that value occurs. In the case where x is continuous, then the sum needs to be replaced with an integral. So, that's the mean or expected value. Another thing that will mention a lot is the empirical mean, and that is simply the average of the data points. Okay, so it's the sum of the data points divided by the number of points. Hmm, so what is the connection between the empirical mean and the definition of mean that we have up here? Okay, so let's look at that a little bit more closely. So, let's say our dataset consists of endpoints x1 through xn. One thing that we can define is what we call the empirical distribution, and that's literally the distribution of the dataset. Okay, so it's the distribution that is uniform over the points that we've seen. So, let's call it P sub n. It puts probability mass 1/n at each of these points x1 through xn. So, the empirical distribution puts probability mass 1/n at each of the data points x1 through xn. And so, once we've defined this empirical distribution, what we can see is that the empirical mean is just the mean of this distribution. So, the empirical mean is the mean of P sub n, and that's the connection or one connection between the empirical mean and this definition of mean. So, there's one other connection that's worth bearing in mind. So, we have these end data points, x1 through xn, let's say that they come from some distribution P. So, let's say they're from distribution P. If we have lots and lots of data then what we'd expect is that the average of these data points is actually pretty close to the mean of P. So, one way to say that is that the empirical mean, the average of the data points, gets closer and closer to the mean of the underlying distribution as we get more and more data points, as n grows. So, this is the connection between the several notions of mean. Okay, so the mean is a common way of summarizing a bunch of numbers by a single number. And another common way of doing this is to take the median. Okay, so let's look at a couple of examples. So, let's look at the first one over here. We have a bunch of numbers, what is their median? Well, in order to figure out the median, what we do is to first put the numbers in order. Okay, so let's see, the smallest is -20, then we have 10, then 20, then 50, then 100. So, we put them in order, and then we take the number in the middle, so in this case, the median is 20. Let's do another one. We start by putting the numbers in order, so we get 10, 20, 50, 60, 90, and 100. And now, we take the number in the middle. But in this case, there are two numbers in the middle, 50 and 60, so which one do we take? It turns out that either of them qualifies as the median, 50 is a median, 60 is a median, and so is anything between them, 51 is a median, 55 is a median. Now, when you give a bunch of numbers to a computer program and ask for a median, it wants to return a single number, so what it typically does is to just return the midpoint. In this case it would return 55. But technically, anything in the range from 50 to 60 qualifies as a median of this data. Okay, so with this in mind, how would we define the median of a random variable? Okay, so what we've said is that it's not something that's uniquely defined. So, in fact, it is any number, it is any value, m, such that the probability that x is greater than or equal to m is at least 1/2 and the probability that x is less than or equal to m is at least 1/2. That's how we define the median. Okay, so to understand this definition a little bit better, let's go back to one of these examples, let's look at this one for example. So, here we have six data points, and the empirical distribution is the distribution that puts probability mass 1/6 at each of them. Okay, so it gives the point 10, the value 10, a weight of 1/6, it gives the value 20 a weight of 1/6, then so on. And in this case, let's say that our random variable x comes from this empirical distribution. Okay, so let's look at the value 50 for example. What is the probability that x is greater than or equal to 50? Well, it could be 50, 60, 90, or 100, so that's four out of six, so the probability that it's greater than or equal to 50 is 2/3. What's the probability that x is less than or equal to 50? Well, it could be 50, 20, or 10, so that's three out of six, that's 1/2. And you can see that both of these probabilities are at least 1/2, and therefore 50 is a median of the data. And likewise, you can check 55 and 60. Okay, so given a collection of data, we have these two different ways of summarizing them by a single number, we have the mean and we have the median. When would we use one rather than the other? Well, at some level, the mean is a little easier to deal with. There's a simple formula for it, it's very easy to compute. The median can be computed efficiently, but it's a little bit more work. So, in some sense, the mean is just a more convenient estimate to use. The problem with the mean though, the bad case for the mean is that it can get thrown off by outliers. And let's see a little toy example of how that might happen. Okay, so let's say that there is some fictional neighborhood in which there are 100 houses and we are looking at the values of these houses. So, let's say that almost all of them, 99 of these houses cost between 100,000 and 300,000. Okay, so that's 99 out of the houses. And then, there's one house that is some sort of palatial mansion that costs $100 million. So, what is the median of these values? The median of the values is 200,000, and that really is very representative of the home cost. The mean, however, is much larger, it's over a million. Okay, so the mean is over a million whereas the median is 200,000. So, this is a case where the mean is completely swayed by this one extreme value, and as a result is not at all representative. So, in situations where there is the possibility of having a few values that are unusually large or uncommonly small, it is often a better idea to use the median even though it's computationally a little bit more involved. So, the mean and the median are good ways of summarizing the center of a bunch of data points or the location of a cloud of points. For this reason, they're both called location estimators. The mean and median are location estimators. What they don't tell us about is the scale of the data or the spread of the data. Okay, so over here, for example, we have two different distributions. They're both centered at the same place, which we are denoting mu over here, so they have the same mean. Because they're symmetric for these distributions, the mean will be the same as the median, and it's at mu. So, the means are the same, but the two distributions are clearly very different. And what differs about them, the most apparent difference, is that they're at very different scales, this distribution over here is much more spread out. So, how do we capture this spread? And the usual way of doing that is by looking at the variance of the distribution. So, here's how the variance is defined. Let's x be a random variable, then the variance of x or equivalently, the variance of the distribution of x is given by this formula. It is the average squared distance from the mean. Okay, so this quantity over here is how far x is from the mean, mu is the mean of x. And then, we square that and then take the average. So, the average squared displacement from the mean, that's the variance. Now, it turns out that there's another way to compute the variance, it's also equal to the expected value of x squared minus the mean squared. And in fact, often the second formula is a little bit more convenient. Anyway, the two of them are always equivalent. And one key property of the variance is that it's always greater than or equal to zero, basically because of this squaring here. So, when is the variance zero? Well, the variance of x is gonna be zero if x does not vary at all. In other words, when x always takes on the same value, when x is constant. Now, one thing about the variance that is a little unintuitive is that it's the average squared distance from the mean. Whereas, what might make more sense or be more intuitive is just the average distance from the mean. So, to correct for that, we often look at the square root of the variance, and that's what we call the standard deviation. And the standard deviation is really a very nice and widely used measure of the spread of a distribution. 