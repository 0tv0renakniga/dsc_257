(light electronic music) (air whooshing) - Okay, welcome back. So far, we have talked about Lp norms, which are widely used but are ultimately a very restrictive class of distance functions. Ultimately, you can write down any distance function you like. And so the question is: Are there some guidelines you should follow when cooking up a distance function for your particular application? One rough guideline is to choose a distance function that is a metric. This is a very broad notion of what it means for a distance function to be well-behaved. And so let's talk about what this is. So let's say that we have data that lies in some space, script X. Now, this could be any space at all. It could consist of vectors, like m-dimensional space, Rm, or it could be a space of images or strings or sounds, anything at all, a space that contains data objects. And now what we're gonna do is to write down a distance function between such objects, okay? So what does that mean? So a distance function takes any two data items, say x and x prime, and returns some distance between them, you know, like 4.2, okay? So in generality, a distance function is a function that takes a pair of items and returns a real number, okay? That's what we mean by a distance function. Now, we'll say that this distance function is a metric if it satisfies four basic properties. First of all, it should be nonnegative. That is, the distance should never be a negative number. The second is that the distance between x and itself should be zero. And the only time the distance between x and y should be zero is if x and y are the same. It shouldn't be the case that there are two different objects and the distance between them is zero. The third property is called symmetry. It says that the distance from x to y should be the same as the distance from y to x. And the final property is called the triangle inequality. And it says that if we have three objects, x, y, and z, then the distance from x to z should be at most the distance from x to y plus the distance from y to z. Okay? So these are four very basic properties, four properties that define roughly what it means for a distance function to be well-behaved. So let's start by looking at Lp norms. It turns out that every Lp norm is actually a distance metric, okay? For example, let's look at L1 norm. So the L1 distance between two vectors, x and y, let's say the vectors are in m-dimensional space, is just the sum over all the coordinates of the absolute difference along that coordinate. And we're saying that this is a metric. How do we know that? Well, we have to check the four properties. So property number one: nonnegativity. Is this nonnegative? Yes it is, because of the absolute value sign. Okay, so we satisfy nonnegativity. Is it the case that the distance between two things is zero only if the two things are identical? That's also true, because if we have two vectors, x and y, that are not identical, if they're not the same, then there is some coordinate along which they differ. And when we look at that coordinate, we'll get some number greater than zero for it. And so the overall L1 distance between the two vectors will be greater than zero, okay? So we satisfy the second property. Is this distance function symmetric? Is the L1 distance between x and y the same as the L1 distance between y and x? That's also true. And that's simply because the absolute value of xi minus yi is the same as the absolute value of yi minus xi. Okay, so because of that, we get symmetry. And finally, does it satisfy the triangle inequality? That also works, okay? And the reason for that is that if we take three vectors, x, y, and z, and we look at one of their coordinates, say, the ith coordinate, then the absolute value between xi and zi is at most the absolute value between xi and yi plus the absolute value between yi and zi. This is just a property of absolute value. And now we can sum over all the i's. So we sum over all i from one to m. And when we do that, we get the sum over all i from one to m of xi minus zi is at most the sum over all i of xi minus yi, absolute value, plus the sum over all i of yi minus zi, absolute value. And this is exactly the triangle inequality. Okay? So L1 distance satisfies the four metric properties, and therefore, it's a metric. And in fact, this holds for any Lp norm. Now let's move to a more interesting example. Let's say that our data space consists of DNA sequences. And we want to define a distance function on these kinds of sequences, okay? So what's a suitable distance function? Well, let's say that we have two sequences, and I'll, of course, DNA sequences are very long. Let's just look at a toy example where let's say the sequences are ACGTGC and y is CGAGC. What is the distance between these two sequences? How should we define it? Well, one very naive definition is to just say, "Let's look at the number of positions on which they differ." And if you do that, well, it looks like they differ on every single position, okay? And so we would say the distance then between them is six. But this is not a good distance function from a biological point of view. A more reasonable distance function that takes into account the biology is to say that the distance between x and y is the number of little edits that you need to make to x to get to y, okay? So we start at x. We have ACGTGC. And the first edit we make is to delete the A. Okay, so we get CGTGC. And now we wanna make it look like y, and so the next edit we make is that we substitute T with A. So substitute T with A. And we get CGAGC. So we just need to make two edits to get from x to y, and so we would say that the edit distance between x and y is two. And this is a much more reasonable notion of distance from a biological point of view. So the question is: Is this a metric? So let's start by defining this distance a little bit more precisely. What we're saying is that the edit distance between x and y is the minimum number of edits we need to get from x to y. And the edits that we are allowed are that we can insert a character, we can delete a character, or we can substitute one character with another. Okay, so it's the minimum number of insertions, single-character insertions that is, deletions, and substitutions needed to get from x to y. Good, so now we have a precise definition of the distance function. And so we can answer the question: Is it a metric or not? And it turns out it is, okay? And once again, the way we check that it's a metric is to simply go through the four properties and check them one by one. So let's do that. So the first property is that it should be nonnegative. Is that true? Yes it is, okay, and in fact, not only is it nonnegative, it's always an integer. The edit distance is gonna be zero, one, two, something like that, okay? So it cannot possibly be a negative number. The next property is that the distance between two objects should be zero if and only if the objects are identical. Okay, you shouldn't have two distinct objects that have zero distance between them. And in this case, if we have two strings that are distinct, the edit distance between them is not gonna be zero. If they are distinct, we need to do some edits to get from one to the other, okay? So that property is also satisfied. The third property is symmetry. We want the distance between x and y to be the same as the distance between y and x. And this one's a little bit more tricky, because the way we've defined edit distance, it's the number of operations to get from x to y. And so the question is: Is this the same as the number of operations needed to go in the opposite direction, from y to x? And indeed, it is, just because the operations are all reversible. So if we are doing an insertion in one direction, we can do a deletion in the opposite direction. If we are doing a substitution in one direction, we can do the reverse substitution in the opposite direction. So in fact, this is true, because the edit operations are reversible. And the final property we need to check is the triangle inequality, which says that the distance from x to z is at most the distance from x to y plus the distance from y to z. In other words, the number of edits to get from x to z is at most the number of edits to get from x to y plus the number of edits to get from y to z. And that's also true, because if we have a way to get from x to y, and we have another series of edits that goes from y to z, then we can just compose those, we can just concatenate them. We first do the edits from x to y. Then we do the edits from y to z. And that gives us a way to go from x to z. Okay? So edit distance satisfies these four properties, and therefore, it's a metric, which is great. So why is that great? What is the benefit of having a distance function that's a metric? Well, from our point of view, from the point of view of unsupervised learning, it turns out that many of the methods that we study in this class are really designed exclusively for Euclidean distance. But there are also many methods that work for any distance function that's symmetric. And this includes methods for fast proximity search, methods for clustering, and so on. So as long as our distance function is a metric, we can use any of these. And so for example, if our distance function is edit distance, we would be able to use these procedures and estimators. Now, frequently, we will be using one distance function, the KL divergence, that's not a metric, okay? So we've seen this distance function before. Why is it not a metric? Well, let's look at the four properties. Is it the case that the KL divergence is always greater than or equal to zero? Yes it is, okay? So so far, so good. Is it the case that the KL divergence between two distributions is zero if and only if the distributions are equal? That's also true, okay? So the first two metric properties are just fine. Is it the case that the KL divergence between p and q is the same as the KL divergence between q and p? In other words, does KL divergence satisfy the symmetry property? No, it does not. What about the triangle inequality? Is it the case that the KL divergence between p and some other distribution r is at most the KL divergence between p and q plus the KL divergence between q and r? No, it doesn't satisfy that either. So KL divergence violates two of the defining properties of a metric, okay? So it's not a metric. And yet, it has a lot of other useful properties, which his why we end up using it quite extensively. So we've talked about metrics, and the reason we introduced this is because, in unsupervised learning, distance functions are really very important. And so it has been of interest to come up with methods for unsupervised learning that work for broad classes of distance functions, okay? And in particular, there are a lot of unsupervised learning methods that work for any distance function that's a metric, okay? But people have also looked at other classes of distance functions. One of them, for example, is the class of Bregman divergences. And this is a very different class of functions that includes squared Euclidean distance and KL divergence. And there's a bunch of methods that work for any distance that's a Bregman divergence. And likewise, there's another family called the F-divergences, okay? So in order to achieve some kind of generality in unsupervised learning, in order to define estimators and procedures that work not just for one distance function but for a whole host of them, people have been looking at these kinds of large families of distances. And we'll return to this topic later on. 