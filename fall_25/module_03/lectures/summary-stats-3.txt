(upbeat music) (slide whooshes) - Since we've been talking about the mean and the variance, let's also look at some of their key properties. So very often we'll be looking at linear functions of a random variable. Okay, so for example, let's say we have a random variable X. A linear function of the random variable is something like two X plus three. And the question is, what is the expected value of this linear function and what is its variance? In order to figure this out, let's look at a few examples. So first of all, let's say we have a set of numbers and we double all of them. What happens to the average? Well, if we double the numbers, the average also gets doubled. What about the variance? So we're doubling the numbers, that means the scale of the numbers is doubled. So the standard deviation is doubled. The variance is the square of the standard deviation. So if we double the standard deviation, it means the variance gets multiplied by four, okay? So if we double a set of numbers, the mean is doubled, and the variance is quadrupled. Okay. Now, on the other hand, let's say that we take our set of numbers and instead of doubling them, we just increase all of them by one. So we sort of shift them over by one. What happens to the average? Well, if everything's shifted over by one, the average also goes up by one. So the mean, we do a plus one. What about the variance? Well, we've shifted the numbers over, but the spread of the numbers hasn't changed at all. So the variance is unchanged. Okay, so this gives us enough information to figure out what happens in a case like this. So what is the mean of two X plus three? Well, it's twice the mean of X, that's the doubling, plus three. That's the shifting over. What about the variance? Well, the plus three doesn't change the variance, but the doubling multiplies the variance by four. And so in this way, we can relate the mean and variance of this linear function of the random variable to the mean and variance of the original random variable. So let's write this out in slightly greater generality. So we start with some random variable X, and now we have some constants, A and B, and we define a new random variable which is this linear function AX plus B. And in our example above A was two and B was three, okay? So what is the expected value of V? Well, it is A times the expected value of X plus B. And what is the variance of V? Well, the plus B doesn't change the variance, it's just a shift. When we multiply X by A, it means that the standard deviation is multiplied by A, so the variance is multiplied by A squared. So the variance of V is A squared times the variance of X. Okay, so this is the effect of the linear function on the mean and variance. Now so far we've just talked about a single random variable. What if we have a whole bunch of random variables, X one through XM? It turns out that the expected value of the sum, okay so the average value if you will, of X one plus X two all the way to XM, is just the sum of the individual expected values. Okay, so the average value taken on by X one plus X two plus X three all the way to XM is just the average value of X one, plus the average value of X two, plus the average value of XM. And the remarkable thing is that this holds always, even if there are all sorts of complicated dependencies between the X one through XM. Okay, and so this property is a very powerful property called linearity of expectation, and this is gonna turn out to be a very useful property for us. What about variance? Is there a linearity of variance? Okay, so let's be a little bit more concrete about this. We've seen that if we have two random variables, X and Y, then by linearity of variance no matter what X and Y are, even if they're dependent, the expected value of X plus Y is just the expected value of X plus the expected value of Y. Is this also true of variance? Is it the case that the variance of the sum X plus Y is just the variance of X plus the variance of Y? It turns out that in general it's not the case. Okay, so for example, let's say that Y is equal to X. Okay, so they're dependent. Y is in fact identically equal to X. In that case, the variance of X plus Y is just the variance of X plus X, which is the variance of two X. And we've seen that doubling, quadruples the variance. Okay, so this is four times the variance of X. And that's not the same as the variance of X plus the variance of Y, which is just twice the variance of X, okay? So when there are dependencies between random variables, we don't necessarily have linearity of variance. On the other hand, if X and Y are independent it is the case that the variance of X plus Y equals the variance of X plus the variance of Y. So the high level summary is that we have linearity of expectation always, and we have linearity of variance when the random variables are independent. 