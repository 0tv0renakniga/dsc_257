(gentle music) - In a lot of unsupervised learning, we get to choose the representation of the data and a distance or similarity function on the data space. So let's talk a little bit more about these. Last time we introduced the ChemCam data. And here is, again, a picture of a ChemCam observation. It's a wavelength spectrum. Along the horizontal axis, we see wavelengths, okay? So these start at 240 and they go up beyond 820. And along the vertical axis, we have the intensity at that wavelength. Now, in tabular form, the data looks like this. So these are the wavelengths again, and this is a single observation, this column over here, and the length of the column is 6,144. That's the number of different wavelengths that are measured. And so if we wanted to, we could represent an observation by simply copying down this column. And if we did that, then the data space would be R, that is to say real values, to the 6144. So vectors consisting of 6,144 real values. So this is one way we could represent the data, but there are many alternatives, and we could turn to something else if this particular representation turns out to be problematic for some reason, and indeed it is problematic. So one problem with this representation is the issue of scaling. So consider these two observations over here, two different wavelength spectra. They have spikes at exactly the same places. So this is the same wavelength as this, and this is the same wavelength as this, and this is the same as this, and so they represent exactly the same chemical composition. What differs, however, is just the intensity values measured at these wavelengths. Now, we should really be thinking of them as identical. And yet in the 6,144 dimensional vector space, these two observations are very far apart. And in fact, the Euclidean distance between them will be significant. This is a problem. How do we fix this? So one very simple way of doing that is to simply normalize the observations, okay? And for example, we could normalize them to add up to one. The way we would do that is that if we have an observation, X, okay, and in this case it will have 6,144 entries. We could replace X by a normalized version where we divide each coordinate by the sum of all the coordinates. So after we do that, what do we get? So first of all, the coordinates are still positive numbers. They're either zero or something larger, and now they add up to one, they have the same sum, and so these two observations will become equal. They'll map to exactly the same vector. The other interesting thing about these vectors is that, let's say they're positive numbers and they add up to one, and so we can think about them as probability distributions. These normalized vectors are probability distributions, that is to say distributions over wavelengths, distributions over 6,144 outcomes. So now we have a modified input space. Our input space has gone from 6,144 dimensional vectors to the probability simplex, over 6,144 outcomes. Now, the notation we're gonna use for the probability simplex is this. Delta sub-M is the space of all distributions over M outcomes. Okay, in our example, we have 6,144 outcomes, but more generally, we represent a distribution by a vector where we say, what is the probability of outcome number one, P1, what is the probability of outcome number two, all the way to the probabilities of outcome number N, okay? And because they're probabilities, these need to be greater than or equal to zero and they need to add up to one. So this space is the probability simplex, and it's a kind of data that we'll be working with a lot. So we have now modified our data representation. Previously we had R to the 6144, but we found that it was problematic. So we said that instead we are gonna look at data in delta to the 6144. We are gonna convert the observations into probability distributions, and that takes care of the scaling problem. Now that we've changed our representation, now that we have a new data space, script X, we can also look at different distance functions. Since we're dealing with probability distributions, what are some distance functions that are popularly used with probabilities? Well, one of these is L1 distance. Now, L1 distance is something that works for arbitrary vectors. They don't have to be probabilities, okay? But it's something that's quite popular with probability distributions. So let's say we have two vectors, X1 through XM and Z1 through ZM, so vectors X and Z. Then the L1 distance between them has these two vertical bars along with this subscript one. And to compute the L1 distance, we look at the difference between the two vectors along each individual coordinate. We take the absolute value of that distance, and we add these up. That's the L1 distance between X and Z. So as I said, L1 distance can be used for arbitrary vectors, but it's also a very popular distance function in the specific case where the vectors are probability distributions, so let's look at an example of that. So here we have two distributions, P and Q, over four outcomes. P is the distribution shown in blue and Q is the distribution shown in orange, and the outcomes are outcome number one, outcome number two, outcome number three, and outcome number four. Since there are four outcomes, each of these distributions can be represented by a vector with four numbers, okay? So P, for example, gives probability 0.1 to outcome number 1, probability 0.4 to outcome number 2, probability 0.2 to outcome number 3, and probability 0.3 to outcome number 4, okay? So that's distribution P. And likewise, distribution Q is 0.1 for outcome 1, 0.7 for outcome 2, 0.1 for outcome 3, and 0.1 for outcome 4. Okay, so notice that both of these vectors add up to one and they consist of numbers that are non-negative. Okay, so what is the L1 distance between P and Q? Well, we could use this formula over here, but visually it's actually quite simple. We just look at the difference in heights between them. So over here, there's no difference. Over here the height difference is 0.3. Over here the height difference is 0.1. And over here the height difference is 0.2. And so the L1 distance is 0.3 plus 0.1 plus 0.2, which is 0.6. The L1 distance between these two distributions is 0.6, nice and intuitive. So here's a question. What do you think is the largest possible L1 distance between two distributions? What's the biggest it can get? So that's something to think about. I'll give you the answer, which is two, and maybe you can think about what distributions P and Q would have an L1 distance of two between them, the largest possible L1 distance between two probability distributions. Okay, so we now have a modified data space, the probability simplex, and we said that one of the distance functions we can consider is L1 distance. What are some other distance functions that are popularly used between probability distributions? Well, another distance function, and one that we will be talking about a lot in this class, is the Kullback-Liebler divergence, also known as the KL divergence or relative entropy. So if you have two distributions, P and Q, and they are distributions over M outcomes, the KL divergence or relative entropy is given by this rather strange-looking formula over here. Okay, so let's see what it would be in our previous example. So what are the P and Q that we had? So our P was 0.1, 0.4, 0.2, and 0.3. So 0.1, 0.4, 0.2, and 0.3. And the Q we had was 0.1, 0.7, 0.1, and 0.1. So the KL divergence between P and Q, let's see. So we are summing over all the outcomes, okay? So we're gonna do outcome number one, then outcome number two, then number three, then number four. Let's start with outcome number one. So P sub-1 is 0.1, and then we do log of P sub-1, which is 0.1 over Q sub-1, which is 0.1. Then we move to outcome number two. P sub-2 is 0.4, log 0.4 over 0.7. Now we move to outcome number 3, 0.2, log 0.2 over 0.1. Then we move to outcome number 4, 0.3 over 0.1. And whatever this works out to, that is the KL distance between P and Q. It's a bit of a strange distance function admittedly, but we'll be using it a whole lot in this class, and it's one of the standard distance measures between probability distributions. Okay, so we changed the representation of the data and now we've looked at some other distance functions. Are we all done or are there other problems that we need to solve? Well, there's another potential problem. What if it turns out that the ChemCam instrument is a little bit noisy and that wavelengths, the wavelengths we're measuring bleed in to neighboring wavelengths? Okay, so for example let's say that the spikes should look like this, but instead they are just slightly corrupted so that this spike moves here and this one moves slightly to the left and this one moves slightly to the right. Okay, there's just a little bit of noise in the measurement. So they should be the same observation, but again, in the space of probability distributions, the outcomes have changed. This outcome over here is different from this outcome. They are next to each other, but they are different outcomes. And as a result, in the space of probability distributions, these two vectors are quite far apart and the L1 distance between them is significant, as is the KL divergence. How can we fix this problem if this turns out to be an issue? Well, we only have two things to play with. We can play with the distance function or we can play with the representation, and in fact, either of them could be used to solve this problem. Okay, so let's start by looking at a different distance function. So this is something that's also quite popular called the Earthmover distance or the Wasserstein distance. And so rather than define it formally, let's just see what it would be in the example that we had before. Okay, so we had these two distributions, P and Q. P is blue and Q is orange, and what we are gonna do now is think of each distribution as basically piles of dirt. Okay, so distribution P has one pile of dirt at location number 1, and the height of the pile is 0.1, and then it's got another pile of dirt at location two, and the height of that pile is 0.4, and then it's got another pile of dirt at location three with a height of 0.2, and then a fourth pile of dirt at location four with a height of 0.3, so 4 piles of dirt, okay? And similarly, Q also has four piles of dirt. Now what we want to do is to move the dirt so that P becomes Q. Okay, so we want to move from P to Q, and this is gonna be the Earthmover distance. So the Earthmover distance from P to Q, let's see what we have to do. Okay, so at location one, we're all set. Okay, the piles of dirt have exactly the same height, but at location two there's a discrepancy. And the way we can fix that is by taking 0.1 units of dirt from location 3 and shipping them here, and taking 0.2 units of dirt from location four and shipping them here. Okay, so we gotta ship two piles of dirt. We ship 0.1 units of dirt from location three to location two, so a distance of one, and we ship 0.2 units of earth from location four to location two, so two units. Okay, a distance of two, and so the Earthmover distance is gonna be 0.5. So this is a distance function that doesn't just treat different outcomes as different, but looks at how far those outcomes are from each other. Okay, and this would be one way of resolving noisy measurements. So for example, let's say that we had two wavelength spectra where one was just a slightly corrupted version of the other, okay, where they were off by just a little epsilon. Then the Earthmover distance between these two observations would be, at most, epsilon. Okay, so Earthmover distance would solve this problem. Now, that's how we could solve the problem of noise by choosing a different distance function, but we could equally solve it by choosing a different representation, okay? Because if it's the case that our instrument is a little bit noisy, what it means is that we just don't have enough resolution, we don't have the precision to support this kind of resolution. And so what we can do is to simply bend together collections of wavelengths or perhaps to apply a blurring process to this, and this would also solve the problem. So what I wanna emphasize over here is that, typically, there's a lot of flexibility in how we choose the data representation and also how we choose any distance or similarity function that we attach to the data, and these choices are very important. Getting them right makes all the difference in the outcome of unsupervised learning. 