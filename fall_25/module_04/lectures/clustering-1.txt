(upbeat music) (slide whooshes) - We have been talking about different ways of summarizing data, including simple summaries like the mean and variance. Now we move on to a much more sophisticated type of summary, clustering. And we'll begin with the most popular clustering method there is, K-means. In clustering, the goal is to take a collection of data points and divide it into groups. Typically, we also want to assign a representative to each of these groups. How can we formulate this as an optimization problem? The most common way of doing this is as the K-means problem. So let's take a closer look at it. In K-means, the input consists of N data points, say in D-dimensional space. And the input also includes an integer K, which is the desired number of clusters. The output is the clustering of these endpoints, or more precisely, representatives for each of the clusters. So these representatives, which we'll call mu one, mu two, all the way to mu K, are also points in D-dimensional space. And the intention is that each of the data points will be associated with its closest representative. So for example, for data point XI the closest representative is the representative mu J for which the distance from XI to mu J is minimized. And so the distance from XI to the closest representative is given by this formula over here. It's the minimum from J equals one to K, so the minimum over all K representatives of the distance from XI to that center. And notice that we are using squared Euclidean distance. We can think of this distance as the approximation error of replacing XI by this representative. Now what we wanna do is to find a placement of the centers mu one through mu K that minimizes the total distance from each point to the closest center. Okay, so what we want to minimize is this cost function over here, the sum over all data points of the distance from that point to its closest center where we are using squared Euclidean distance. Okay, so now let's look at a quick example of this. Let's say that we have this data set over here which consists of 10 points in two dimensional space. So we have 10 points, that means N equals 10. The points line in two dimensional space, so D equals two. And let's say that the number of clusters we want is three. Okay, so we ask for K equals three. What's a reasonable clustering of this data? Well, one reasonable way to cluster it is to have a cluster like this, a cluster like this, and a cluster like this. And if those are the clusters, then the representatives or centers of the clusters would be one representative somewhere there, one representative somewhere there, and one representative somewhere there. Okay? And so in this case, mu one, mu two, and mu three would look something like this. Maybe this is mu one, maybe this is mu two, and maybe this is mu three. Now if these are the centers, then the approximation error associated with this data point is the distance to its closest center. So it's that distance squared. And similarly, the approximation error for this point is this distance squared. And the total K-means cost of these three centers, of mu one, mu two, mu three, is the sum of all these distances. So it's the sum of these three squared distances, plus these three distances, plus these four distances. When you add up those 10 squared distances, you get the K-means cost associated with these three centers. Now, is this the best possible placement of the three centers? Eyeballing the data, it does look like the best possible placement, but it's hard to be sure. Okay. Now, once we've chosen three centers, mu one, mu two, and mu three, every point in space gets associated with its closest center. As a result, the clusters are convex regions of the underlying space. And so for example if a new data point shows up and the point lies here, it will be associated with this center. The cluster associated with this center consists of all points of space. It consists of the region of space for which that center is the closest one. Okay, so we've talked a little bit about the K-means optimization problem. How do we solve this problem? How do we find the best possible placement of the K centers mu one through mu K? So the most basic question is what kind of a cost function is this? Does the cost function look like this, something nice and convex? In which case we can reasonably hope to find the optimal solution. Or does it look something like this, something that has lots of local optima, which would be much more difficult? Unfortunately, it looks like this one. And in fact, K-means is an NP-hard optimization problem. What that means is that there is no efficient algorithm that always returns the optimal solution, the optimal placement of the centers. Another way to think of this is that if you have an efficient algorithm for K-means, then there are gonna be some instances, some datasets, on which it returns suboptimal clusterings. This sounds like pretty bad news, but in fact this is a fairly common situation in unsupervised learning and it's something that we are reconciled to in some degree. It's something that we have made peace with, okay? And so what we need to do is to find a good heuristic for solving K-means. Now, by far the most popular method of solving K-means is by Lloyd's algorithm. So this algorithm was developed by Lloyd in the 50s and 60s, and it is by far the most popular way of solving K-means. So much so that it is commonly called the K-means algorithm, okay? Although of course it is just one possible heuristic for solving K-means. Now, Lloyd's algorithm is a local search procedure. What that means is that it starts with some initial guess for where the centers should be, for what mu one, mu two, mu three, mu K should be, and then it tweaks that guess, it moves the centers a little bit, so as to reduce the cost. And then it tweaks the solution a little more. It moves the centers around a bit more to reduce the cost further. And in this way, it keeps tweaking the centers, keeps reducing the cost, until it settles at some local optimum, and that's when the algorithm converges, okay? So let's see an example of how this might work. So let's say we have this same data set over here and that we ask for three clusters. So what we need to do first of all, is to initialize the three centers mu one through mu K. And a common way of doing this is to just pick three of the data points at random. Okay, so let's say that we end up picking these three points. These are our initial centers, mu one, mu two, and mu three. Now, if these are our centers, then our clusters look like this, okay? So for example, we have this cluster over here on the left. Hmm, now if that's the cluster, then it would be better to move the center over here to the mean of those four points. That would lead to a lower K-means cost. Similarly, if we have a cluster over here, then it would be better to move the center somewhere over here, to a more central location to the mean of those five points. So let's do that. Let's move our centers. So we move them, and this is better. It has a lower K-means cost, okay? So now we've completed one iteration of Lloyd's algorithm. And we started with three initial centers and now we've tweaked them to get something better. Now if these are the centers, then the clusters look like this, okay? And let's look at this cluster up here. Hmm, if the cluster consists of those three points then it would be better to put the center there. And let's look at this cluster. Well if that's our cluster, then it would be better to move the center here. And so in the next round of Lloyd's algorithm we move the centers to the means of their current clusters. So we move the centers here. And now we've completed the second round of Lloyd's algorithm. So we have our new placement of the centers, which has even lower cost than before. Now if these are the centers, then our new clusters look like this. And at this point, the algorithm has converged. There is no further change to the centers and we have arrived at a local optimum of the cost function. Okay? So that's Lloyd's algorithm. So one thing to notice about this algorithm is that it is completely deterministic apart from the initial choice of centers, apart from the way in which we initialize those centers. And so initialization is really what determines which local optimum we end up in. The way we initialize in this way determines the quality of the final clustering that we get. Let's see a little example of how this might play out. So here we have two copies of exactly the same dataset and we're gonna run K-means on both of them. But what we are gonna do is to initialize them differently. Okay, so let's start with the one on the left. Let's say that in both cases we want say three clusters. Okay, so on the left we initialize by picking three of the data points at random. And let's say that the data points we choose are this one, this one, and this one, okay? So we picked three data points at random, now let's run Lloyd's heuristic, okay? So initially the cluster associated with this center is gonna be this one. It's the set of all points for which that is the closest center. And the cluster associated with this center will be this one. And the cluster associated with this center will be this one, okay? Now if those are the clusters, well then this center should be moved over here. And this center should be moved here. And this center should be moved somewhere here. And those are the updated centers. Now, what happens is that the algorithm converges, there are no further changes, okay? And so this is the solution returned by Lloyd's algorithm. Now. let's look at the second duplicate copy of the dataset. And once again, we'll run K-means on it. And once again, we'll initialize by picking three of the data points at random. Let's say that this time the data points we end up picking are this one, this one, and this one, okay? So the initial clusters are this, this, and this. And in the first round of Lloyd's algorithm this center over here gets moved over here, to the mean of its cluster. And this center over here gets moved somewhere here, to the mean of its cluster. And this center gets moved probably somewhere like that, the mean of its cluster. And once again, the process converges immediately. There is no further change. So notice that we get completely different solutions in these two cases. Which is the better solution? Well, the way we are evaluating solutions is by the K-means cost. And in this case, the cost is significantly lower for this solution, so it is the better solution. Initialization really matters when running K-means. Okay, well if initialization matters so much, how should we initialize? So let's talk about some common practices. Perhaps the most common thing to do is to simply initialize the centers by picking K of the data points at random. Another option is to use extra centers. So let's say that you want 10 clusters. What you can do is to start by initializing 20 centers. Choose 20 of the data points at random as the initial centers, run K-means so you end up with 20 clusters, and then you merge clusters that are close together so that you get 10 clusters in the end. Okay, so that's another strategy. A third method, which is a little bit more sophisticated and is extremely effective, is called K-means++. Now, this initialization strategy initializes the K centers mu one, mu two, through mu K, by picking them one at a time in a sequential and adaptive way. So it starts by picking mu one. And the way it does that is just by picking mu one at random from the dataset. When it's time to pick mu two, it again picks mu two at random from the data, but now it gives a higher weight to points that are far away from the first center. And when picking mu three, it gives higher weight to points that are far away from the first two centers, and so on. Okay, so it is an adaptive and sequential way of picking the initial centers. Let's see an example of how this might work. Let's say we have a data set that looks like this. And let's say that we want four centers. So our first center is chosen completely at random from the data points. Every point is equally likely to be chosen. Let's say we end up with this as our first center, mu one. Now, when we are picking the second center, we again choose a random from the entire data set. But now the probability of picking a point depends on the squared distance to the first center. So this point, for example, is very close to the first center. That squared distance is rather small. And so this point has a low probability of being chosen, whereas this point over here has a much higher probability. So again, any of these points could be chosen, but we are more likely to pick a point that's far away. And let's say that we end up picking this point, for example, let's say that's our second center. Now it's time to pick our third center, okay? Once again, we choose at random from the whole data set, but now each point is weighted by its squared distance to the closest center so far. And so for example this point, again, has a low weight. Now this point also has a low weight, because it's close to that center. Whereas this point has a much higher probability of being chosen because it is so far from the two existing centers. Let's say we end up choosing this as our third center. And likewise, our fourth center is chosen at random based on the squared Euclidean distance to the three centers we've already chosen. And maybe the fourth one will be something like this. So that's the K-means++ initialization method. A very effective way of initializing K-means for Lloyd's algorithm. Okay, so we've talked a lot about K-means, about how we can find a reasonable clustering. Next time we'll talk about uses of clustering. 