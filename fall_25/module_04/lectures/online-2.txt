(gentle music) (air whooshes) - Welcome back, everybody. Okay, so here's the problem we need to solve. We are operating in the online model of computation where we have an infinite stream of data, x1, x2, x3, and so on. We aren't allowed to store all of it. In fact, we're just allowed to store a constant number of data points, say 100 of them or 1,000 of them. And what we want to do is to maintain a random subset of all the points we've seen so far, a random sample. How can we do this? So to be a little bit more precise, let's say that, at any given time, we want a random subset of k of the points that we've seen so far. K could be something like 10 or 100 or 1,000. So even though we might've seen millions or billions of points, we want to always just keep 10 or 100 of them. How do we do this? So we can start with the simplest case when k is equal to one. In other words, we just want to keep around one point that we've seen so far, chosen at random. So let's call that point s. At time one where the only data point we've seen is x1, s has to equal x1. There is no other choice. So we know how to initialize x. But now new data starts to arrive. And at time t, we get data point x of t. How should we update s? Should we take xt or not take xt? Well at time t we've seen t data points. So the probability of taking xt, of making xt our one random sample should be one over t. Okay, so let's just say that our update will be the following, with probability one over t set s equal to xt and that's it. That's our random sampling algorithm. Now this seems intuitive, but we do need to establish that it's correct. So what is it exactly that we're trying to show? So what we want to say is that at any time t, this value that we have s is truly a random sample of everything we've seen so far. In other words, s is equally likely to be x1 or x2 or x3 all the way up to xt. At any time t, our value s is equally likely to be any any of x1, x2, all the way to xt. Okay. So the probability that s is any one of these values should be exactly one over t. Okay, so that's what we hope to establish. So let's go ahead and do this a little bit more formally. Okay, so what we want to show is that s has a probability one over t of being any of the values x1 to xt. Okay, so pick any time t and any of the data points that we've seen up to that time. So any j less than or equal to t, what we want to say is that the probability that s is xj at time t should be exactly one over t. Okay, so at time t, s is equally likely to be x1 or x2 or x3, all the way up to xt. Okay, so let's go ahead and check this. Let's do a little proof. What is the probability that x is xj at time t? Well let's look back at our algorithm. Okay, so in order for s to be xj at time t what needs to have happened is that when xj arrived, we actually took it. So we chose xj and then we did not take any of the subsequent points. Okay, so we chose xj, but we did not choose xj plus one and we did not choose xj plus two and we didn't choose xt, okay? So s is equal to xj if we chose xj but we didn't choose xj plus one, didn't choose xj plus two, and didn't choose xt, okay? That's the only way in which we can end up with xj as our random sample. Okay, so what's the probability that when xj arose, we chose it? Well at that time, the time was j, and so the probability of choosing it was one over j. Okay, now, when xj plus one arose the probability that we didn't choose it was one minus one over j plus one and when xj plus two arose, the probability that we didn't choose that was one minus one over j plus two, and so on. And when xt arose, the probability of choosing it was one over t. And so the probability of not choosing it was one minus one over t. So we have this expression over here. How can we simplify this? Well, so the first term is just one over j. The second term can be simplified as j over j plus one. The third term can be simplified as j plus one over j plus two, and so on. And the last term is t minus one over t. And now look, everything seems to cancel out. This cancels out with that, this cancels out with that. This will cancel out with that. This will cancel out with the previous thing. And so we just have one over t. And so indeed the probability of having xj at time t is exactly one over t, okay? And this holds no matter what j is. So this is exactly the property we want. And so we have formally established that our algorithm for having one random sample, for maintaining one random sample is in fact correct. Okay, so now let's move to maintaining k random samples. And we'll start with the case where we want to maintain random samples with replacement. What that means is that our first random sample, let's call it s1, should be chosen at random from the entire list of everything we've seen so far. And our second random sample should again be chosen at random from the entire list of everything we've seen so far, and so on all the way to our kth random sample. And since they're all chosen at random from the entire list it's quite possible that there would be duplicates, okay? So that's what we mean by with replacement. Now at time one, the only data point we've seen is x1 and therefore all k of our random samples, s1, s2, all the way to sk have to equal x1. Okay? There's no choice. But now time moves along and we see x2, and x3 and x4, and so on, and we update our random samples along the way. So let's see what happens at time t. At time t, we get a new data point, x of t, what update should we do? Here's our update. For each of our random samples, s1 through sk, we'll set it to this new point xt with probability one over t. Okay, and so we do this for each sj separately. So with probability one over t, we set x1 to xt, with probability one over t, we set s2 to xt, and so on all the way to sk. So this is also a nice little algorithm, but why is it correct? Well, if we look at it closely, it looks a whole lot like our previous algorithm. In fact, to be precise, this is exactly like running k independent copies of our earlier algorithm. So this is exactly the same as running k independent copies in parallel of our earlier single sample algorithm. And that's exactly what we want. K random samples with replacement is exactly the same as k independent single samples. Great. So now let's move on to the final case, which is k random samples without replacement, and what that means is that our first random sample should be chosen at random from everything we've seen so far, whereas our second random sample is chosen at random from everything we've seen so far apart from the first random sample. And our third random sample is chosen at random from everything we've seen so far apart from the first two that we've chosen, and so on. Okay, so in this way we avoid duplicates. So this is a slightly more intricate case, which is why we've left it till the end. But let's see how this works. So at time k, we've seen exactly k distinct items. And so our random samples have to be those items, x1 through xk. So that's simple enough, but now time goes on and at time t we get x of t. Should we take it or not? Well, let's see. So at time t, we've seen a total of t elements, x1 through xt, and we are only gonna store k of them. So the probability that xt is one of the k that we choose should be k over t. Okay, so we'll say that with probability k over t, we will take xt. Okay, but where should we put xt? Should we put it in s1? Should we put it in s2? Should we put it in sk? Well, we just choose at random. So choose some value between one and k at random and set sj equal to xt. Okay, so also a very simple algorithm, and in fact, this correctly maintains k random samples without replacement. Why is this correct? Well, the argument is a little bit more intricate and it's something that you can try on your own if you feel so inclined. Okay, great. So now we have algorithms to do random sampling in the online model of computation and we can do random sampling with replacement or without replacement. So with this primitive, let's go back to the problem of computing the median in the online framework. And we talked a little about this before and it seemed like a really hard problem. And so we settled upon what seemed like an approximate solution. We said, well, let's say that we just maintain a random sample of everything we've seen so far, maybe of 1,000 things that we've seen so far and then whenever we want the median, we just compute the median of the things we actually have in memory of the thousand points in memory, okay? And we'd return that as the median. Now, clearly that's not gonna be the exact median of everything we've seen so far. We could easily have seen millions or billions of points but hopefully it's an approximate median. And so now let's go ahead and clarify in what sense exactly this is true. Okay, so here's a little result that we can show and let me help you parse this result. So start by choosing any delta and epsilon and you should think of these as just some small numbers like 0.1. Okay? So these are small constants. Now, if the number of random samples we maintain k is at least this much, okay, so whatever that is, whether it's 1,000 or 5,000, if we maintain at least that many random samples, then these approximate medians, these values m sub t, the median of the random samples we have will be very close to the true median in the sense that this value will be a half plus or minus epsilon fractile of the data that we've seen so far. Okay, so let's see exactly what this means. So first of all, what's a fractile? Well, a fractile is exactly the same as a percentile but divided by 100. Okay, so for example, the median is the 50th percentile and when we express that as a fraction, we would say it's the half fractile. Okay, so a fractile is just the same as a percentile but scaled by 100. So now let's say that we have a value of epsilon that's small, let's say 0.01. So let's say epsilon equals 0.01, then our approximate median is a half plus or minus epsilon fractile. That means that our value m sub t is between the 0.49 fractile and the 0.51 fractile of the data we've seen so far. So then m sub t is between the 0.49 fractile between the 0.49 and 0.51 fractile of the data we've seen so far. In other words, it's between the 49th percentile and the 51st percentile. And that's the sense in which we have an approximate median. That's a pretty good guarantee. So instead of getting exactly the 50th percentile we have some value that's between the 49th percentile and the 51st percentile. So we can see that random sampling is indeed a very useful primitive and the fact that we can do it very simply in the online model of computation is a big help. 