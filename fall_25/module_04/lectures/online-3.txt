(upbeat music) - Okay, so we have been talking about the online model of computation in which there is an endless or very large stream of data that we see one point at a time. We're only allowed a small amount of memory, and so we certainly can't store all the points that we've seen. Nonetheless, we've seen how in this model we can compute the mean, and we can also maintain a random sample of all the points that we've seen. Now let's turn to something a little different for the sake of variety. Let's look at the problem of heavy hitters. Heavy hitters are data points that occur many times in the stream. For example, in web search there are certain super popular queries that are made over and over again, and search engines are very interested in keeping track of these heavy hitters. How can this be done? So let's begin by formalizing the problem. There is a very large stream of data, X one through xm. Here M is the size of the stream. You should think of it as some enormous number that is unknown in advance. There's also a parameter epsilon which is a value between zero and one. And we define heavy hitters as elements whose frequency is at least epsilon M. That is elements that occur at least epsilon M times in the data stream. Okay, so these are the elements that we call the heavy hitters. So for example, if epsilon equals 0.01, then a heavy hitter is an item that is repeated at least 0.01 times M times in the data stream. In other words, these heavy hitters constitute 1% of the entire data stream. So what we wanna do is to keep track of these heavy hitters and also keep track of their frequencies. So first off, how much space is it gonna take to just write down the identities of the heavy hitters and their frequencies? So the first question is how many heavy hitters can there be? Well, let's say epsilon equals 0.01. That means that every heavy hitter is at least 1% of the data stream, which means that there can be at most 100 heavy hitters. More generally, for a general parameter epsilon, the number of heavy hitters is at most one over epsilon. Okay, so the number of heavy hitters can be anything up to one over epsilon. And this is because each heavy hitter is repeated epsilon M times, and the total number of items in the stream is M. Okay, so the amount of space needed is one over epsilon because that's the number of heavy hitters. And for each of these heavy hitters, we have to write down the identity of the heavy hitter, and we have to write down the number of times it occurred. So let's say that the items themselves lie in a space of size N. Then writing down the identity of any given item takes log N bits at best. What about the frequencies? Well, the total number of items in the stream is M. So the frequency of any one item is anything from zero to M. And to write down a number in this range we need log M bits. So we can see that just writing down a list of the heavy hitters along with their frequencies requires about this much space. And what we'd like to do is to design an online algorithm that uses only about this much space. It seems like a tall order. It's not something that seems easy at all, yet it turns out that there's an extremely simple algorithm for this. So the way this algorithm works is by maintaining a hash table T of size K, and you should think of K as being roughly one over epsilon. Okay, so it maintains the hash table, and the items in the hash table are data items, okay? And with each element in the table, we have an associated value V of X, which is an integer in the range one, two, three and so on. And intuitively, V of X is the number of times that item has occurred in the stream so far. Okay? So we have this hash table that's a relatively small size, only size K. So we can't store a lot of items in it. Whatever items we have in the table, we have an associated count, V of X. Okay, so think of V of X as a count of the number of times X has occurred in the stream so far. And it's not gonna be exactly correct, it's gonna be an approximate count. As an approximate count of the frequency of item X so far of the number of times X has so far appeared in the data stream. Okay. So let's see how this works. So initially the table is empty because we haven't seen anything yet, and now the clock starts ticking, and at each time T we get the next data point X sub T, okay? And now you should think of this stream where there are lots of repeats, okay? So if X sub T is something that's already in the table, well, then we just increment its counter, okay? So we just increment the counter if it's already in the table, very straightforward. If it's not in the table, but there is enough space in the table for another item, then we add it to the table, and we set its counter to one. Again, very reasonable, okay? So we've got this item X sub T. If it's already in the table, we increment its counter. If it's not in the table, and there's a little bit of space left in the table, then we put X sub T in the table, and we give it a count of one. The third case is the hard case. This is the case where X sub T is not in the table, but the table is full, okay? And what we do here is a little bit strange. So the table is full. We don't have enough space to insert X sub T. Instead, we just look at everything in the table, and we decrement their counters. And if anything's counter now drops to zero, we remove it from the table, okay? So this is the final step, and this is the one tricky step in the algorithm. Okay, so to reiterate, we have a table of a few of the items that we've seen so far. We have seen lots and lots of different items. We can't possibly store all of them, okay, so we just maintain a list of a few of them. And for the ones that we have, we have these approximate counts that we're calling V. When we get a new item, if it's already in the table, we increment its count. If it's not in the table but there's space for it, we include it and give it a count of one. But if it's not in the table, and we don't have space for it we do something a little bit tricky. And another way to think of this step that will be useful a bit later is the following: Let's say that the table is already full, and does not contain X sub T. What we will do is we'll add X sub T to the table. So now the table has grown a little bit beyond its bound. Okay, so we've grown the table a little bit more than we should have. We add X sub T to it. We give it a count of one, and now we decrement everything in the table, okay? And at that point X sub T gets decremented to zero, and therefore we'd remove it from the table. So if we like, we can pretend that at this point over here we actually did add X sub T to the table. It's just that by this point, it will have been removed because we added it to the table. It had a count of one. We decremented the counts, and so it got removed. Okay? So based on this, what we can see is that our counters are at most the actual number of times the item occurred. Because of the decrement step, our counts might be a little bit too small. Okay? So what can we show about this algorithm? What guarantees does it have? Well pick any time T, and let's say that the number of times any given item X appeared in X one through XT is frequency sub T of X. Okay, so that's the number of times X has occurred up to time T. This guarantee holds for every single X. The count associated with X, V of X, is at most the frequency of X. Okay, and we've seen that directly from the algorithm, but we also know that these counts can actually be too small because we occasionally decrement the counts. What can be shown is although the counts might be too small, they're not too small by very much. They'll be too small by only that amount, T over K plus one. And this holds not just for the items in the table, but for all items. If an item is not in the table, we just take V of X to be zero. Okay, so anything that's not in the table has got an implicit value V of X of zero, and the same formula holds for it. Okay, So this is a very strong guarantee, and in particular, if we take, if we set K to be some constant times one over epsilon, what we can see is that all heavy hitters will be in the table, and their frequencies will be approximately correct. So all heavy hitters will be in T with approximately correct frequencies. So this algorithm does indeed solve the heavy hitters problem. Now why does this guarantee hold? Why is this true? So the argument is a little bit tricky, but let's just go through it relatively quickly. So we are gonna think of V of X as holding the number of times X has occurred so far. And indeed, most of the time, the algorithm is really trying to do that, okay? The only thing that it's doing that's a little bit off is that once in a while, K plus one of these values are decremented. Okay, that's the only thing that it's doing that detracts from V of X holding the number of occurrences of each item, okay? So once in a while, you have these decrement steps where K plus one of these values get decremented. But the key point is that there can't be very many of these decrement steps because during each decrement step, K plus one values get decremented. The K values in the table and the new X sub T that we implicitly added to the table. Okay, so during a decrement step, K plus one values get decremented. And since by time T we've only seen a total of T counts, okay, since the total count at time T is only T, the maximum number of decrement steps is T over K plus one. Okay? So each count is approximately correct. It's at most the correct frequency, and the number of decrement steps is at most this much. And so the count can be off by at most this much. Okay, so it's a tricky argument, but nonetheless we have this very simple and effective algorithm for maintaining heavy hitters. So that's all we'll say about the online model of computation for now. But throughout this course, whenever we introduce method after method for unsupervised learning, one question that we'll always have at the back of our minds is can this be done online? 