(light music) - Okay, welcome back. We have talked in detail about the k-means optimization problem and Lloyd's heuristic for solving the problem. So we know how we can cluster data, but what do we actually do with these clusterings? Well, clustering is very widely used and its many use cases can be divided into two broad categories. The first is finding meaningful structure in data. So for example, if there's a company that offers a streaming service and has data on its many customers about the kind of content that they're streaming, this could be a truly enormous collection of data that is impossible to fathom in its entirety. So a useful thing to do then is to cluster customers according to their usage patterns and get a representative for each cluster. This could be a very insightful summary of this enormous data set. So in a situation like this, if there is a natural group in the data, for example, a collection of customers that have very coherent, very similar behavior patterns, we would really like to be able to recover this group. In other words, if there are natural groups or salient groups in the data, we want to be able to capture them. This is the sort of situation that arises in exploratory data analysis. Now, a very different use of clustering is for vector quantization. The situation here is that we have a large space, possibly infinite, and we want to discretize this space. We want to quantize it. We want to find a collection of representatives and what we are gonna do is to map every single point in this space to its closest representative. And our hope is to do this in a way that minimizes the amount of distortion. For example, in audio coding, every point in space is a piece of sound, for example, a speech snippet of a certain length. So this is a large space. In fact, it's an infinite continuous space and we wanna discretize it. We want to digitize it. We want to find a bunch of representatives which in this context are sometimes called code words. And then what's gonna happen is that every speech snippet will be associated with its closest code word and will be discretized by replacing it by that code word. So this operation, replacing a speech snippet by the closest code word induces a certain amount of distortion. And our entire goal is to choose a set of representatives that minimizes this distortion. It is completely irrelevant whether there are natural groups in the data or not. So these are two rather different uses of clustering. Let's go ahead and see an example of each of these starting with vector quantization. So here's a question. How can we represent images as vectors of fixed length, after all images come in many different shapes and sizes. Is there a way to standardize them to represent them as vectors of some fixed length so that we can apply typical machine learning methods to them? Now, many different proposals have been made for this and let's talk about one of them that's based on k-means. So here's the idea, let's start by looking at this image. What we are gonna do is to look at patches in the image of a fixed size. Okay, so a patch is a little rectangular area like this and let's look at patches of some fixed size, say L by L. And we are gonna look at all such patches. So there's one patch, here's another patch, here's another patch, here's another patch. This is another patch. Here's another one, and so on. Now, what is the size of each patch? Well, each patch is L by L, so it has L squared pixels in it and the pixels are in color. So each pixel is represented by three numbers, R, G, and B. So the total size of a patch is 3L squared. We can write down a patch as a vector of dimension 3L squared. Okay, so each patch is a vector of dimension 3L squared. Okay, how many patches are there in this image? Well, there's lots of them. Roughly the number of patches is the number of pixels because we can associate each patch with its upper left corner, okay? And so each patch has a unique upper left corner. And so the number of patches is roughly the number of pixels. There are some boundary effects that come into play. Okay, so that's image patches. Now, let's say we have a large collection of images, okay? So we take our first image, image number one, and we extract all the patches from it, okay? So we get lots and lots of patches from that image and every single patch is a vector of the same size. Every patch is a vector of size 3L squared. Now we take our second image and we extract all the patches from that one, and so on. Let's say we have N images. So we go up to image number N and we extract all patches from that image. So we have a large collection of images, and from each image we have extracted all patches. And so now we have a truly enormous collection of patches and each patch is a vector of size 3L squared. So we take this enormous collection of patches and we apply k-means clustering to it. Okay, so we are applying k-means to this very large collection of vectors of size 3L squared. What do we get back? Well, we get back k-centers we get back center mu one all the way to center mu k. And each center is also a vector of size 3L squared. In other words, each center is a patch. Okay, great. So now we have k-patches, k-centers. What we are gonna do is to use these centers to encode any image of any size as a vector of length k. How can we do that? Well, let's go back to this image over here. Let's look at this patch and let's say its closest center is center number 23. So its closest center is mu sub 23. We can now label that patch 23. Why? Because the patch comes from cluster 23, its closest center is number 23. Let's look at this patch. Maybe its closest center is number five. Let's look at this patch over here. Maybe its closest center is again, 23, because it looks kind of like the other patch. In this way, every patch in this image gets a label, gets a number from one to k. Now we want to convert this image into a vector of length k. How do we do that? Okay, so we're gonna create a vector of length k, entries one, two, all the way to k. The first entry is gonna be the fraction of patches in the image, whose label is one, okay? Whatever that fraction is, maybe 0.1. The next entry, entry number two is gonna be the fraction of patches in that specific image, whose label is two, the fraction of patches that belong to cluster number two, maybe that fraction is zero. And let's say entry number 23 is gonna be the fraction of patches that belong to cluster number 23. And this might be a larger fraction because these are the grassy patches and there seem to be a lot of them in this image. Okay, so in this way, we represent the image as a vector of dimension k, and in fact, this vector is also a probability distribution. The entries are probabilities that add up to one. Okay, so this is a rather ingenious use of k-means in order to encode arbitrary size images as vectors of fixed length. So at a high level, what happened here? Well, what we really did was to take every image and think of it as a bag of patches to think of an image as a collection of patches of fixed size, and then we used k-means to quantize the space of image patches. Now, let's look at another kind of application, one in which we are looking for natural groups in data. For this, we are going use the animals with attributes dataset. So this is a small dataset that has information about 50 animals like the antelope, the grizzly bear, the beaver, and so on. For each animal there are 85 features, and these features cover the animal's appearance and behavior. For example, does the animal have a tail? Does it swim? Does it live in the desert? And so on. For each feature, there's a value in the range zero to 100. Okay, so we have 50 animals and for each of them there are 85 features. So we have 50 data points in 85 dimensional space. Let's use k-means to cluster these 50 animals. What value of k should we use? Well, there's no correct answer, but let's shoot for 10 clusters and see what we get. Now, if you recall in Lloyd's algorithm, the final clustering that you get is heavily dependent upon the initialization. And so what we've done over here is to look at two different random initializations and see the clusterings that we get in each case. Okay, so let's look at the one on the left, for example, these are the 10 clusters that we got. So zebra is in a cluster all by itself. Then there is a cluster that contains spider monkey, gorilla, and chimpanzee. That sounds pretty sensible. There's a cluster with beaver and otter, also fairly sensible. And so you can see these are the 10 clusters that we get. The second time we ran k-means, we got these 10 clusters. Okay, how do these compare? Well, some of the clusters are actually the same as before. So zebra again is by itself, spider monkey, gorilla, and chimpanzee are again in a cluster but there are some differences. This time, for example, there's a cluster consisting of grizzly bear and polar bear. So what can we make of this? Well, these clusters are certainly informative and sensible. Neither of them is perfect. And in fact, this is fairly typical of what happens when we use clustering for exploratory data analysis. We get a bunch of clusters that are often fairly sensible and helpful, although they are rarely exactly the clusters that an expert would have chosen. 