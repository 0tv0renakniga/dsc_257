(bright music) - Now we've been talking a lot about k-means for the simple reason that it is the most popular method of clustering, but does it have any disadvantages? And if so, are there any alternatives? The huge advantage of k-means is that it is simple and efficient and easy, it is also very good at vector quantization. Indeed, the k-means optimization problem precisely captures the distortion that's induced when data points are replaced by their closest representatives. But when it comes to using clustering for exploratory data analysis, when it comes to situations when we are looking for natural groups in data, k-means can be a little bit more questionable. And the reason for this is that it ultimately wants clusters to be roughly spherical and roughly of the same size. Let's look at a small example to see why this is the case. Suppose we have data that looks something like this. So that's our data dataset. What are the natural groups or natural clusters in this data? Well, there's one cluster over here and there's another cluster over here, and the centers of these clusters are here and here. But if we were to give these centers to k-means, k-means would insist on splitting the clusters somewhere here. It simply would not allow a wide cluster, like the one on the left, to sit right next to a compact cluster, like the one on the right. And this is problematic. k-means roughly wants its clusters to be approximately the same size, and there's simply no reason why naturally occurring clusters in data would be like that. So what can we do about this? Are there other algorithms, other methods, that are simple and easy, just like k-means, but at the same time, are more flexible in the kinds of clusters that they permit? A prime candidate for such a method is the mixture of gaussians. And this is something that we'll just give a little preview of right now. We'll be returning to this a bit later in the course. In the mixture of gaussians, each cluster is represented by a Gaussian distribution. And as a result, you can get clusters like this, clusters that are a very varied shape and size. Each cluster is given by a Gaussian, so a mean, which is the center of the cluster, and a covalence matrix, which gives each cluster its characteristic ellipsoidal shape. Each cluster also has a mixing weight, which says what fraction of the dataset belongs to that particular cluster. When we put these together, these Gaussian distributions and these mixing weights, we actually get a distribution over the entire data space. And so in this way, the mixture of Gaussian solves two problems at the same time, clustering as well as density estimation. And it turns out that you can learn a mixture of gaussians from data using an algorithm that's very similar to Lloyd's heuristic for k-means. So this is a very appealing method of clustering and we'll be returning to this in a few weeks a bit later on in the course. 