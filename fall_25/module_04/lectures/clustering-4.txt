(lively music) (screen whooshing) - So far, we've been talking exclusively about flat clustering where all the clusters are at the same level. Now let's talk about hierarchies of clusters. Okay, so here's a simple dataset. How many clusters are there in this data? Well, let's see. It looks like there are two clusters, one over here and one over here. Or maybe there are three clusters where this is a cluster and this is also a cluster. What's the right answer? Well, actually, both of them are correct. When you look at a picture like this, you see that in general, it's impossible to talk about the correct number of clusters. Much of real-life data is like this. It contains cluster structure at multiple different scales at multiple different granularities and this is where hierarchical clustering is useful. It allows you to capture cluster structure at all possible scales simultaneously. In the case of this data, for example, a hierarchical clustering would be a tree where at the top of the tree, you'd have a node representing a single cluster containing all of the points. This node would have two children where one child represents this cluster on the left and the second child represents this cluster on the right. The right-hand cluster would have two further children, one representing this cluster in here and another representing this cluster and so on. This would keep splitting until at the very bottom, there would be clusters containing just individual data points. Hierarchical clustering methods were initially developed by biologists who wanted to use them to reconstruct evolutionary trees. More recently, they've been used heavily in understanding gene expression data. So let's look at this matrix, for example. Over here along the vertical, we have genes, 6,000 genes roughly, and along the horizontal, we have samples from different patients. So each particular entry in this matrix, each particular pixel tells us how strongly that particular gene was expressed in that particular sample. And the values range from dark red to dark green. This is a huge amount of data. One thing we can do to understand it better is to cluster the genes. So what this would involve is thinking of each row as a data point. So in this case, we would get 6,000 rows and then we could apply a standard hierarchical clustering method to these rows. When we do that, we get this tree over here and an interesting phenomenon emerges. It turns out that there are lots of genes, for example, these over here that are co-expressed that have very similar expression levels. This is a huge benefit because it means that this bewildering number of genes, 6,000 of them, can be replaced by a much smaller number of gene groups that act as a unit, as a module. And the hierarchical clustering gives us these modules automatically. Now, another thing we could do with this same dataset is instead of thinking of the rows as being the data points, we can think of the columns as being the data points. In this case, what we are clustering is different samples. So for each of these samples, we have now a 6,000 dimensional vector, a very high dimensional vector, and we can cluster those. In this particular study, the patients were all suffering from a particular type of cancer, and it turned out that some of them were responding well to treatment while others weren't. And this was a little bit puzzling, but when the patients were clustered using hierarchical clustering, it turned out that there were two broad clusters and that those two groups of patients were actually in clusters that were very well separated in 6,000 dimensional space. The researchers concluded that there were actually two subtypes of this cancer and that's what determined whether one patient responded well to treatment or not. So how do we do hierarchical clustering? Well, there's a variety of algorithms for it. Let's start with perhaps the simplest algorithm, which is known as single linkage. So here we have a dataset of 10 points. Single linkage is a bottom-up algorithm. It constructs the tree in a bottom-up manner and at the bottom of the tree, you have each data point by itself in a singleton cluster. So at the very bottom, we have these 10 clusters and now we're gonna build a tree by progressively merging clusters. So the first thing we do is to merge the two closest clusters, which are probably these. So we begin by merging four and five. Then we merge the next two closest, which look like they're probably six and seven and then the next two closest, which seem to be two and three. Now what comes next? Probably eight and nine. Now, the next merger will probably be to merge one into this other cluster because this distance is very small. So we merge one into the cluster containing two and three and we end up with a cluster over there. The next merger would probably be to actually put all of these together. The next merger would bring 10 into the cluster consisting of eight and nine. Then we join these clusters and finally, we would put everything together. So this is the resulting hierarchical clustering. This kind of tree is often called a dendrogram and it contains cluster structure at all possible levels. If you want very coarse cluster structure, you can just break the top of the tree. If you want slightly more refined structure, you can go further down the tree. So it's a very flexible and useful summary of the data. Now, it turns out that there's a whole set of different linkage methods that all follow roughly the same blueprint. So they all begin with each data point in a cluster by itself, and they all build the hierarchy in a bottom-up manner by progressively merging clusters. Which pair of clusters get merged at each stage? The two closest clusters. So the only thing distinguishing different methods is how you measure the closeness between two clusters. So let's say that we're building up this hierarchy, and at some stage, we have created two clusters, C and C prime. How do we measure the closeness between these two clusters? What measure of distance do we use between two sets of points between two clusters? For the single linkage method, we take the distance between these two clusters to be the distance between the closest pair of points in them. So single linkage considers the distance between these two clusters to be this distance. There's another method called complete linkage, which considers the distance between these two clusters to be the distance between the furthest pair of points in them, which would be perhaps this distance over here. So these are two extremes, two extreme ways of measuring the distance between two clusters. What is most common is to use some average notion of the distance between these two clusters. These are generically called average linkage methods and they're the most popular methods for hierarchical clustering. There are three common variants. The first takes the distance between two clusters to simply be the average inter-point distance between those two clusters. So if the clusters are C and C prime, it looks at every point in C and every point in C prime and looks at the difference between those two points and then takes their average. So it's literally the average inter-point distance between the two clusters. Another method of average linkage is to do something even simpler, is to say, just look at the mean of cluster C and the mean of cluster C prime and see how far apart they are. That's what we're gonna call the distance between the two clusters. The third method looks a little bit more complicated. This is the formula for it. What does this mean? Well, what it is is the increase in k-means cost occasioned by merging cluster C and C prime. So let's see what this means. So we have some cluster C, which has a bunch of points in it, and we have a cluster C prime that has another set of points in it. Now, C by itself has a certain k-means cost. What is the cost? Well, the center of C is somewhere here and the k-means cost associated with C is just the sum of these distances. Likewise, the cost associated with C prime is the sum of those distances, square distances to be precise. Now, when we merge C and C prime, we get a single cluster and we're gonna have to assign a single center to all of those points. As a result, the k-means cost is gonna go up. This formula over here captures exactly the amount by which that k-means cost goes up when we merge the two clusters. And what we wanna do is to repeatedly merge the clusters that lead to the smallest increase in k-means cost. This is called Ward's method and it may well be the most popular type of linkage method for hierarchical clustering. Well, that's it for our lightning overview of clustering. Hopefully this has been enough to give you a little bit of a sense of what this area is all about. 