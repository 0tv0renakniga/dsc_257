(lively music) (screen whooshing) - Welcome back, everybody. Now, in unsupervised learning, it is often the case that we need to run algorithms, such as clustering on datasets that are truly enormous. Sometimes the datasets are just too large to fit into memory. Other times, the data form a never-ending stream. For example, readings from environmental sensors that just go on forever. In situations like this where the datasets are enormous or endless, it's useful to operate in the online model of computation. So let's see what that is. So in the online model, there is an endless, or at any rate, very large stream of data, x1, x2, x3, and so on. And at each point in time, we just get the next data point and we use it to update our model, whatever it is that we are learning. And that's it. The data point then goes away. The only space we're allowed is roughly the amount of space we need to store our model. So in particular, we're not allowed to store all the data we've seen so far. This seems like a very difficult model. After all, typical learning algorithms usually assume that the dataset is entirely in memory, and that we have the ability to iterate over the data again and again. We don't have any such luxury. We just get to see each data point once and then it goes away forever. What can we possibly compute in such a constrained model? Let's start with a simple example. So let's see how we can compute the mean in an online model. So we have an endless stream of data, x1, x2, x3, and so on. And at any given time, we want to maintain the mean, the average of all the points that we've seen so far. How can we do this? So let's store the mean. Let's denote the mean so far by mu, we'll initialize it to zero, and now the data start arriving, x1, then x2. So when we get xt, how should we update the mean? So, so far, we have the mean of the first t minus one points. Now xt has arrived and we somehow need to average it in. How should we update mu? Well, here's an update that works. We can set mu to one minus one over t times its current value plus one over t times the new data point that's arrived. So a very simple update. Why does this work? Well, let's see. What we want is the mean of x1 through xt. Well, what is that? Well, that's just the sum of x1 through xt divided by t. So it's x1 plus x2, all the way to xt minus one plus xt, all divided by t. And now let's just put a parentheses around these, the first t minus one points because we know that those, the sum of the first t minus one points is just t minus one times the mean of those points. So that's the term in parentheses. And then we add on xt and we divide the whole thing by t. So this is equal to one minus one over t times the mean of the first t minus one points plus one over t times the t-th data point. And that's exactly the update that we have over here. So this simple algorithm does, in fact, correctly maintain the average or mean of all the points we've seen so far. So we can actually get a lot more mileage out of this simple procedure because we can also use it to maintain other expected values. For example, we can maintain the expected value of x squared in much the same way. And if we can do the expected value of x and the expected value of x squared, that means we can also maintain the variance since the variance can be written in terms of the expected value of x squared and the expected value of x. So this procedure is actually quite flexible. So we've been able to do the mean online. How about the median? So let's see. Let's say that we have the median of the first t minus one points. Let's say that somehow we've been able to maintain that and now the next data point arrives, x sub t arrives. How should we adjust the median? This seems very difficult to do, maybe even impossible. Indeed, an online computation of the median is rather challenging. So let's think about a very different approach. Let's say that we were able to maintain a random sample of all the points we've seen so far. So let's say that we could maintain, let's say we were able to maintain a random sample of say 1,000 of the points we've seen so far. So what that means is that let's say we've seen one million data points. So all we maintain in memory is 1,000 of those points chosen uniformly at random. And as time goes on, as we go from one million to two million to three million, at any given point, we still have just 1,000 points chosen at random. What we could do then is any time we wanted the median, we could just return the median of these 1,000 points. It wouldn't be the exact median of all the points we've seen so far but it should be a pretty good approximation. So we could just return the median of these 1,000 points. And our hope is that this is a fairly good approximation to the true median. Indeed, being able to maintain a random sample would be a very useful primitive. And we're gonna turn to that in our next mini lecture. 